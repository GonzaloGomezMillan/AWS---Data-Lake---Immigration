{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Data Lake - Immigration\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is to create a data lake with data about immigration in EEUU, which facilitates the analysis and predictions to several types of company.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import pyreadstat\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id,row_number\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "output_data = \"s3a://bucket-test-udacity/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(date):\n",
    "    \"\"\"\n",
    "    Convert to yyyy-mm-dd format\n",
    "    \n",
    "    :return: date in yyyy-mm-dd format\n",
    "    \"\"\"   \n",
    "    if date is not None:\n",
    "        return pd.Timestamp('1960-1-1')+pd.to_timedelta(date, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()\n",
    "    return spark\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23bf4e12be0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### SCOPE \n",
    "\n",
    "The purpose of this project is to create a data lake with data about immigration in the US and the circumstances in which it has occurred.\n",
    "\n",
    "To carry it out, we are considering three datasets, which are going to be raw data for our data lake.\n",
    "\n",
    "**The datasets used are:**\n",
    "- **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace.  **[This](https://www.trade.gov/i-94-arrivals-program)** is where the data comes from. The National Travel and Tourism Office (NTTO) manages the ADIS/I-94 visitor arrivals program in cooperation with the Department of Homeland Security (DHS)/U.S. Customs and Border Protection (CBP). The I-94 provides a count of visitor arrivals to the United States (with stays of 1-night or more and visiting under certain visa types) to calculate U.S. travel and tourism volume exports.\n",
    "- **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it **[here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)**.\n",
    "- **Airport Code Table:** This is a simple table of airport codes and corresponding cities. It comes from **[here](https://datahub.io/core/airport-codes#data)**.\n",
    "- **World Temperature Data**: \n",
    "\n",
    "The main tools which are going to be used are:\n",
    "- **Python libraries** like pandas or numpy\n",
    "- **Pyspark** to deal with the immigration dataset\n",
    "- **Aparhe Airflow** to automate a pipeline to extract this information programatically and to mantain the database updated.\n",
    "- **Amazon S3**: to store both the raw data and the final data lake in parquet.\n",
    "- **Amazon EMR**: to process the data with PySpark.\n",
    "\n",
    "#### DATA DESCRIPTION\n",
    "\n",
    "- **I94 Immigration Data:** This is a dataset with information from the people which arrive to EEUU as immigrants. \n",
    "\n",
    "- **U.S. City Demographic Data:** The information included in this dataset is the following:\n",
    "    - **City names**\n",
    "    - **State**: \n",
    "    - **Median age**\n",
    "    - **Male population**\n",
    "    - **Female population**\n",
    "    - **Total population**\n",
    "    - **Number of veterans**\n",
    "    - **Foreign born**\n",
    "    - **Average household size**\n",
    "    - **State code**\n",
    "    - **Race**\n",
    "    - **Statistic values**\n",
    "    \n",
    "    \n",
    "- **Airport Code Table:** Dataset with information about different airports. This information includes:\n",
    "    - **ident**: Identification code\n",
    "    - **type**: type of airport\n",
    "    - **name**: name of the airport\n",
    "    - **elevation_ft**: elevation above the sea level\n",
    "    - **iso_country**: iso code of each country\n",
    "    - **iso_region**: iso code of each region\n",
    "    - **municipality**: municipality where the airport is located\n",
    "    - **gps_code**: gps code of the airport\n",
    "    - **iata_code**: An IATA airport code, also known as an IATA location identifier, IATA station code, or simply a location identifier, is a three-character alphanumeric geocode designating many airports and metropolitan areas around the world, defined by the International Air Transport Association (IATA).\n",
    "    - **local_code**: local code of the airport\n",
    "    - **coordinates**: coordinates of the airport\n",
    "    \n",
    "    \n",
    "- **Global land temperatures by city**: Dataset with informmation about the temperature in different cities at different dates.\n",
    "    - **dt**: date of the data\n",
    "    - **AverageTemperature**\n",
    "    - **AverageTemperatureUncertainty**\n",
    "    - **City**\n",
    "    - **Country**\n",
    "    - **Latitude**\n",
    "    - **Longitude**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I94 Immigration Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_immigration_data(url = 'immigration_data_sample.csv'):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "#     df_sas = spark.read.csv('immigration_data_sample.csv', header = True)\n",
    "    df_sas = spark.read.parquet(output_data + 'sas_data/*.parquet')\n",
    "    return df_sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sas = read_immigration_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**U.S. City Demographic Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cities_data(url = 'us-cities-demographics.csv'):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "    df_cities = pd.read_csv(output_data + 'us-cities-demographics.csv', sep=';')\n",
    "    return df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = read_cities_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Airport Code Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_airport_data(url = 'airport-codes_csv.csv'):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "    df_airport = pd.read_csv(output_data + url)\n",
    "    return df_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = read_airport_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global land temperatures by city**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_temp_data(url = \"C:/Users/gonza/Downloads/GlobalLandTemperaturesByCity.csv\"):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "    df_temp = spark.read.csv(output_data + url, header = True, inferSchema = True)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = read_temp_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Although each file and dataset is different and have different problems to solve but the steps to be implemented are the following:\n",
    "\n",
    "* Modify the name of the columns to more descriptive names\n",
    "* Modify data types\n",
    "* Fox the missing values\n",
    "* Drop duplicates values\n",
    "* Replace codes with more descriptive names\n",
    "* Drop unnecesary columns\n",
    "\n",
    "Not all steps will have to be applied to all datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. I94 Immigration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748522.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.498180e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  5748517.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "1  5748518.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "2  5748519.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "3  5748520.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "4  5748521.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "5  5748522.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "\n",
       "  i94addr  depdate  ...  entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      CA  20582.0  ...     None        M   1976.0  10292016      F   None   \n",
       "1      NV  20591.0  ...     None        M   1984.0  10292016      F   None   \n",
       "2      WA  20582.0  ...     None        M   1987.0  10292016      M   None   \n",
       "3      WA  20588.0  ...     None        M   1987.0  10292016      F   None   \n",
       "4      WA  20588.0  ...     None        M   1988.0  10292016      M   None   \n",
       "5      HI  20579.0  ...     None        M   1959.0  10292016      M   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0      QF  9.495387e+10  00011       B1  \n",
       "1      VA  9.495562e+10  00007       B1  \n",
       "2      DL  9.495641e+10  00040       B1  \n",
       "3      DL  9.495645e+10  00040       B1  \n",
       "4      DL  9.495639e+10  00040       B1  \n",
       "5      NZ  9.498180e+10  00010       B2  \n",
       "\n",
       "[6 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sas.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3096313, 28)\n"
     ]
    }
   ],
   "source": [
    "print((df_sas.count(), len(df_sas.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096074</td>\n",
       "      <td>2943721</td>\n",
       "      <td>...</td>\n",
       "      <td>392</td>\n",
       "      <td>2957884</td>\n",
       "      <td>3095511</td>\n",
       "      <td>3095836</td>\n",
       "      <td>2682044</td>\n",
       "      <td>113708</td>\n",
       "      <td>3012686</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3076764</td>\n",
       "      <td>3096313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>3078651.879075533</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.9069344733559</td>\n",
       "      <td>303.28381949757664</td>\n",
       "      <td>None</td>\n",
       "      <td>20559.84854179794</td>\n",
       "      <td>1.0736897761487614</td>\n",
       "      <td>51.652482269503544</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1974.2323855415148</td>\n",
       "      <td>8291120.333841449</td>\n",
       "      <td>None</td>\n",
       "      <td>4131.050016327899</td>\n",
       "      <td>59.477601493233784</td>\n",
       "      <td>7.082885011090295E10</td>\n",
       "      <td>1360.2463696420555</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1763278.099749858</td>\n",
       "      <td>1.9909824761792666E-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.02688853063322</td>\n",
       "      <td>208.5832129278886</td>\n",
       "      <td>None</td>\n",
       "      <td>8.777339474881993</td>\n",
       "      <td>0.5158963131657235</td>\n",
       "      <td>42.97906231370985</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17.420260534588262</td>\n",
       "      <td>1656502.4244925014</td>\n",
       "      <td>None</td>\n",
       "      <td>8821.743471773656</td>\n",
       "      <td>172.63339952061747</td>\n",
       "      <td>2.2154415947557632E10</td>\n",
       "      <td>5852.676345633783</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>5KE</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>..</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>/   183D</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>*FF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00000</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25%</td>\n",
       "      <td>1577601.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>7102016.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3680.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.6035184433E10</td>\n",
       "      <td>101.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50%</td>\n",
       "      <td>3103156.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20560.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>7252016.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3872.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.9360890533E10</td>\n",
       "      <td>408.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75%</td>\n",
       "      <td>4654299.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1.0132016E7</td>\n",
       "      <td>None</td>\n",
       "      <td>3945.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.350973973E10</td>\n",
       "      <td>903.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max</td>\n",
       "      <td>6102785.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>YSL</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>ZU</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>X</td>\n",
       "      <td>YM0167</td>\n",
       "      <td>ZZ</td>\n",
       "      <td>9.991556593E10</td>\n",
       "      <td>ZZZ</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary              cicid                   i94yr   i94mon  \\\n",
       "0   count            3096313                 3096313  3096313   \n",
       "1    mean  3078651.879075533                  2016.0      4.0   \n",
       "2  stddev  1763278.099749858  1.9909824761792666E-13      0.0   \n",
       "3     min                6.0                  2016.0      4.0   \n",
       "4     25%          1577601.0                  2016.0      4.0   \n",
       "5     50%          3103156.0                  2016.0      4.0   \n",
       "6     75%          4654299.0                  2016.0      4.0   \n",
       "7     max          6102785.0                  2016.0      4.0   \n",
       "\n",
       "               i94cit              i94res  i94port            arrdate  \\\n",
       "0             3096313             3096313  3096313            3096313   \n",
       "1   304.9069344733559  303.28381949757664     None  20559.84854179794   \n",
       "2  210.02688853063322   208.5832129278886     None  8.777339474881993   \n",
       "3               101.0               101.0      5KE            20545.0   \n",
       "4               135.0               131.0     None            20552.0   \n",
       "5               213.0               213.0     None            20560.0   \n",
       "6               512.0               504.0     None            20567.0   \n",
       "7               999.0               760.0      YSL            20574.0   \n",
       "\n",
       "              i94mode             i94addr  ... entdepu  matflag  \\\n",
       "0             3096074             2943721  ...     392  2957884   \n",
       "1  1.0736897761487614  51.652482269503544  ...    None     None   \n",
       "2  0.5158963131657235   42.97906231370985  ...    None     None   \n",
       "3                 1.0                  ..  ...       U        M   \n",
       "4                 1.0                10.0  ...    None     None   \n",
       "5                 1.0                40.0  ...    None     None   \n",
       "6                 1.0                99.0  ...    None     None   \n",
       "7                 9.0                  ZU  ...       Y        M   \n",
       "\n",
       "              biryear             dtaddto   gender             insnum  \\\n",
       "0             3095511             3095836  2682044             113708   \n",
       "1  1974.2323855415148   8291120.333841449     None  4131.050016327899   \n",
       "2  17.420260534588262  1656502.4244925014     None  8821.743471773656   \n",
       "3              1902.0            /   183D        F                  0   \n",
       "4              1962.0           7102016.0     None             3680.0   \n",
       "5              1975.0           7252016.0     None             3872.0   \n",
       "6              1986.0         1.0132016E7     None             3945.0   \n",
       "7              2019.0                 D/S        X             YM0167   \n",
       "\n",
       "              airline                 admnum               fltno visatype  \n",
       "0             3012686                3096313             3076764  3096313  \n",
       "1  59.477601493233784   7.082885011090295E10  1360.2463696420555     None  \n",
       "2  172.63339952061747  2.2154415947557632E10   5852.676345633783     None  \n",
       "3                 *FF                    0.0               00000       B1  \n",
       "4                 2.0        5.6035184433E10               101.0     None  \n",
       "5                 2.0        5.9360890533E10               408.0     None  \n",
       "6                 2.0         9.350973973E10               903.0     None  \n",
       "7                  ZZ         9.991556593E10                 ZZZ       WT  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sas.summary().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=0, i94yr=0, i94mon=0, i94cit=0, i94res=0, i94port=0, arrdate=0, i94mode=239, i94addr=152592, depdate=142457, i94bir=802, i94visa=0, count=0, dtadfile=1, visapost=1881250, occup=3088187, entdepa=238, entdepd=138429, entdepu=3095921, matflag=138429, biryear=802, dtaddto=477, gender=414269, insnum=2982605, airline=83627, admnum=0, fltno=19549, visatype=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values analysis\n",
    "df_sas.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_sas.columns]).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in the variables `i94addr=59`, `depdate=49`, `visapost=618`, `occup=996`, `entdepd=46`, `entdepu=1000`, `matflag=46`, `gender=141`, `insnum=965`, `airline=33` and `fltno=8`. It is going to be necessary to analyze each of them to identify a strategy to avoid having missing values, if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_immigration_data(df_sas):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Change column names\n",
    "    2. Change the data types\n",
    "    3. Fix the missing values\n",
    "    4. Drop duplicate values\n",
    "    5. Replace codes with more descriptive values\n",
    "    6. Create new features\n",
    "    7. Create the final dataframes\n",
    "    \n",
    "    INPUT:\n",
    "    df_sas (Spark DataFrame): DataFrame with\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_travel (Spark DataFrame): dataframe with the data related with the considered travels\n",
    "    df_personal_info (Spark DataFrame): dataframe with the data related with the travelers\n",
    "    '''\n",
    "    \n",
    "    # 1. Change column names\n",
    "    # Keys to modify the codes for names easier to understand\n",
    "    names = {'cicid':'immigrant_id','I94YR': 'year', 'I94MON':'month', 'I94CIT':'country_1','I94RES':'country_2','I94PORT':'city',\n",
    "        'ARRDATE':'arrival_date','I94MODE':'transport_mode','I94ADDR':'state','DEPDATE':'departure_date',\n",
    "         'I94BIR':'age_respondent','I94VISA':'visa_code','COUNT':'summary_statistics','DTADFILE':'character_date_field',\n",
    "        'VISAPOST':'department_visa','OCCUP':'occupation','ENTDEPA':'arrival_flag','ENTDEPD':'departure_flag',\n",
    "        'ENTDEPU':'update_flag','MATFLAG':'match_flag','BIRYEAR':'birth_year','DTADDTO':'character_date_field',\n",
    "        'GENDER':'non_inmigrant_sex','INSNUM':'ins_number','AIRLINE':'airline','ADMNUM':'admission_number',\n",
    "        'FLTNO':'flight_number','VISATYPE':'visa_type'}\n",
    "    \n",
    "    # Modification of the column names for others more intuitive\n",
    "    for i in names:\n",
    "        df_sas = df_sas.withColumnRenamed(i,names[i])\n",
    "\n",
    "    # 2. Change the data types\n",
    "    from pyspark.sql.functions import substring, length, col, expr\n",
    "    df_sas = df_sas.withColumn(\"country_1\",expr(\"substring(country_1, 1, length(country_1)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"immigrant_id\",expr(\"substring(immigrant_id, 1, length(immigrant_id)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"year\",expr(\"substring(year, 1, length(year)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"month\",expr(\"substring(month, 1, length(month)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"country_2\",expr(\"substring(country_2, 1, length(country_2)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"arrival_date\",expr(\"substring(arrival_date, 1, length(arrival_date)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"transport_mode\",expr(\"substring(transport_mode, 1, length(transport_mode)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"departure_date\",expr(\"substring(departure_date, 1, length(departure_date)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"age_respondent\",expr(\"substring(age_respondent, 1, length(age_respondent)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"visa_code\",expr(\"substring(visa_code, 1, length(visa_code)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"summary_statistics\",expr(\"substring(summary_statistics, 1, length(summary_statistics)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"birth_year\",expr(\"substring(birth_year, 1, length(birth_year)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"admission_number\",expr(\"substring(admission_number, 1, length(admission_number)-2)\"))\n",
    "    \n",
    "    df_sas = df_sas.withColumn(\"country_1\", col(\"country_1\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"immigrant_id\", col(\"immigrant_id\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"year\", col(\"year\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"month\", col(\"month\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"country_2\", col(\"country_2\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"arrival_date\", col(\"arrival_date\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"transport_mode\", col(\"transport_mode\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"departure_date\", col(\"departure_date\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"age_respondent\", col(\"age_respondent\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"visa_code\", col(\"visa_code\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"summary_statistics\", col(\"summary_statistics\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"birth_year\", col(\"birth_year\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"admission_number\", col(\"admission_number\").cast('int'))\n",
    "    \n",
    "    convert_to_datetime_udf = udf(convert_to_datetime, DateType())\n",
    "    df_sas = df_sas.withColumn('arrival_date', convert_to_datetime_udf(col('arrival_date'))) # Fila nueva\n",
    "    df_sas = df_sas.withColumn('departure_date', convert_to_datetime_udf(col('departure_date'))) #Fila nueva\n",
    "    \n",
    "    # 3. Fix the missing values\n",
    "    # 3.1. I94ADDR (state)\n",
    "    df_sas = df_sas.filter(df_sas.state.isNotNull())\n",
    "    \n",
    "    # 3.2. DEPDATE (departure_date)\n",
    "    df_sas.filter(df_sas.departure_date.isNotNull())\n",
    "    \n",
    "    # 3.3. VISAPOST (department_visa)\n",
    "    df_sas = df_sas.drop('department_visa')\n",
    "    \n",
    "    # 3.4. OCCUP (occupation)\n",
    "    df_sas = df_sas.drop('occupation')\n",
    "    \n",
    "    # 3.5. ENTDEPD (departure_flag)\n",
    "    df_sas = df_sas.filter(df_sas.departure_flag.isNotNull())\n",
    "    \n",
    "    # 3.6. MATFLAG (match_flag)\n",
    "    # There are no missing values anymore, because those ones has been already solved in a prior step.    \n",
    "    \n",
    "    # 4. Drop duplicate values\n",
    "    df_sas = df_sas.dropDuplicates()    \n",
    "    \n",
    "    # 5. Replace codes with more descriptive values\n",
    "    # Read the SAS file with the meaning of the codes of the columns I94CIT & I94RES,I94PORT, I94MODE, I94ADDR\n",
    "    country_codes = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    with open(country_codes) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.replace('\"','').replace('\\n','').replace(\"'\",'') for line in lines]\n",
    "    \n",
    "    df_sas_codes = pd.DataFrame(lines)\n",
    "    \n",
    "    # 5.1. I94CIT & I94RES\n",
    "    values_I94CIT_I94RES = df_sas_codes[9:298]\n",
    "    values_I94CIT_I94RES = values_I94CIT_I94RES[0].str.split('=', expand = True)\n",
    "    values_I94CIT_I94RES.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94CIT_I94RES = spark.createDataFrame(values_I94CIT_I94RES)\n",
    "    df_sas = df_sas.join(values_I94CIT_I94RES, df_sas.country_1 == values_I94CIT_I94RES.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','country_1_name')\n",
    "    df_sas = df_sas.join(values_I94CIT_I94RES, df_sas.country_2 == values_I94CIT_I94RES.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','country_2_name')\n",
    "    \n",
    "    # 5.2. I94PORT\n",
    "    values_I94PORT = df_sas_codes[302:962]\n",
    "    values_I94PORT = values_I94PORT[0].str.split('=', expand = True)\n",
    "    values_I94PORT.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94PORT['code'] = values_I94PORT['code'].str.strip()\n",
    "    values_I94PORT['name'] = values_I94PORT['name'].str.strip()\n",
    "    values_I94PORT.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94PORT = spark.createDataFrame(values_I94PORT)\n",
    "    df_sas = df_sas.join(values_I94PORT, df_sas.city == values_I94PORT.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','city_name')    \n",
    "    \n",
    "    # 5.3. I94MODE\n",
    "    values_I94MODE = df_sas_codes[972:976]\n",
    "    values_I94MODE = values_I94MODE[0].str.split('=', expand = True)\n",
    "    values_I94MODE.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94MODE['code'] = values_I94MODE['code'].str.strip()\n",
    "    values_I94MODE['name'] = values_I94MODE['name'].str.strip()\n",
    "    values_I94MODE = spark.createDataFrame(values_I94MODE)\n",
    "    df_sas = df_sas.join(values_I94MODE, df_sas.transport_mode == values_I94MODE.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','transport_mode_name')\n",
    "    \n",
    "    # 5.4. I94ADDR\n",
    "    values_I94ADDR= df_sas_codes[982:1036]\n",
    "    values_I94ADDR = values_I94ADDR[0].str.split('=', expand = True)\n",
    "    values_I94ADDR.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94ADDR['code'] = values_I94ADDR['code'].str.strip()\n",
    "    values_I94ADDR['name'] = values_I94ADDR['name'].str.strip()\n",
    "    values_I94ADDR = spark.createDataFrame(values_I94ADDR)\n",
    "    df_sas = df_sas.join(values_I94ADDR, df_sas.state == values_I94ADDR.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name', 'state_name')\n",
    "\n",
    "    # 6. Create new features\n",
    "    # 6.1. date_arrival_temp - Key to link this table with the temperature table\n",
    "    # Creation of columns with year and month of arrival to be used as key to link this table with the temperature table\n",
    "    df_sas = df_sas.withColumn(\"year_arrival\", year(df_sas.arrival_date)) # Creation of a column with the year of arrival\n",
    "    df_sas = df_sas.withColumn(\"month_arrival\", month(df_sas.arrival_date)) # Creation of a column with the month of arrival\n",
    "    df_sas = df_sas.withColumn(\"day_arrival\", lit(1)) # Creation of a column with 1s, because only the year and the month are \n",
    "                                                        # going to be relevant for this feature\n",
    "    #Creation of the feature\n",
    "    df_sas = df_sas.withColumn(\"date_arrival_temp\", make_date(df_sas.year_arrival, df_sas.month_arrival, df_sas.day_arrival))\n",
    "    \n",
    "    #Drop the columns which are not useful anymore\n",
    "    df_sas = df_sas.drop('year_arrival')\n",
    "    df_sas = df_sas.drop('month_arrival')\n",
    "    df_sas = df_sas.drop('day_arrival')\n",
    "    \n",
    "    # 6.2. travel_id - Key to identify each of the travels\n",
    "    df_sas = df_sas.withColumn(\"travel_id\",row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#     df_sas = df_sas.withColumn(\"travel_id\", monotonically_increasing_id())\n",
    "    \n",
    "    # 6.3. city_state - Key to link this table with df_cities and df_airport\n",
    "    df_sas = df_sas.withColumn('city_name', split(df_sas['city_name'], ',').getItem(0))\n",
    "    df_sas = df_sas.withColumn(\"city_name\", initcap(df_sas.city_name))\n",
    "    df_sas = df_sas.withColumn(\"state_name\", initcap(df_sas.state_name))\n",
    "    df_sas = df_sas.withColumn('city_state', concat(df_sas.city_name, lit('_'), df_sas.state_name))\n",
    "    \n",
    "    # 7. Define the final dataframes\n",
    "    df_travel = df_sas.select('travel_id','city_state','immigrant_id', 'year', 'month', 'city', 'arrival_date','transport_mode','state',\n",
    "              'departure_date', 'airline', 'flight_number', 'city_name', 'transport_mode_name',\n",
    "              'state_name','date_arrival_temp')\n",
    "    \n",
    "    df_personal_info = df_sas.select('immigrant_id','country_1', 'country_2', 'visa_code','birth_year','non_inmigrant_sex', 'ins_number', 'visa_type', \n",
    "              'country_1_name', 'country_2_name')\n",
    "    \n",
    "    return df_travel, df_personal_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel, df_personal_info = cleaning_immigration_data(df_sas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travel_id</th>\n",
       "      <th>city_state</th>\n",
       "      <th>immigrant_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>city</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>transport_mode</th>\n",
       "      <th>state</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>city_name</th>\n",
       "      <th>transport_mode_name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>date_arrival_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Las Vegas_Arizona</td>\n",
       "      <td>5889208</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>LVG</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>CM</td>\n",
       "      <td>00252</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Air</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Atlanta_S. Carolina</td>\n",
       "      <td>5770290</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>SC</td>\n",
       "      <td>2016-05-06</td>\n",
       "      <td>DL</td>\n",
       "      <td>00392</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Air</td>\n",
       "      <td>S. Carolina</td>\n",
       "      <td>2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Phoenix_Arizona</td>\n",
       "      <td>4052101</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>PHO</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-05-14</td>\n",
       "      <td>BA</td>\n",
       "      <td>00289</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Air</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Phoenix_Arizona</td>\n",
       "      <td>4273097</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>PHO</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>BA</td>\n",
       "      <td>00289</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Air</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>San Francisco_Arizona</td>\n",
       "      <td>5831892</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>SFR</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>00505</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Air</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2016-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   travel_id             city_state  immigrant_id  year  month city  \\\n",
       "0          1      Las Vegas_Arizona       5889208  2016      4  LVG   \n",
       "1          2    Atlanta_S. Carolina       5770290  2016      4  ATL   \n",
       "2          3        Phoenix_Arizona       4052101  2016      4  PHO   \n",
       "3          4        Phoenix_Arizona       4273097  2016      4  PHO   \n",
       "4          5  San Francisco_Arizona       5831892  2016      4  SFR   \n",
       "\n",
       "  arrival_date  transport_mode state departure_date airline flight_number  \\\n",
       "0   2016-04-30               1    AZ     2016-05-08      CM         00252   \n",
       "1   2016-04-30               1    SC     2016-05-06      DL         00392   \n",
       "2   2016-04-22               1    AZ     2016-05-14      BA         00289   \n",
       "3   2016-04-23               1    AZ     2016-05-07      BA         00289   \n",
       "4   2016-04-30               1    AZ     2016-08-01      AA         00505   \n",
       "\n",
       "       city_name transport_mode_name   state_name date_arrival_temp  \n",
       "0      Las Vegas                 Air      Arizona        2016-04-01  \n",
       "1        Atlanta                 Air  S. Carolina        2016-04-01  \n",
       "2        Phoenix                 Air      Arizona        2016-04-01  \n",
       "3        Phoenix                 Air      Arizona        2016-04-01  \n",
       "4  San Francisco                 Air      Arizona        2016-04-01  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_travel.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(df):\n",
    "    '''\n",
    "    Function which checks if there is information in the dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    df - pandas DataFrame: dataframe to be checked\n",
    "    \n",
    "    OUTPUT:\n",
    "    None\n",
    "    '''\n",
    "    \n",
    "    if df.toPandas().size > 0:\n",
    "        print('The dataset has information.')\n",
    "    \n",
    "    else:\n",
    "        df.toPandas().size == 0\n",
    "        print('There has been a failure by processing the file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has information.\n"
     ]
    }
   ],
   "source": [
    "quality_check(df_travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has information.\n"
     ]
    }
   ],
   "source": [
    "quality_check(df_personal_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>immigrant_id</th>\n",
       "      <th>country_1</th>\n",
       "      <th>country_2</th>\n",
       "      <th>visa_code</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>non_inmigrant_sex</th>\n",
       "      <th>ins_number</th>\n",
       "      <th>visa_type</th>\n",
       "      <th>country_1_name</th>\n",
       "      <th>country_2_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5889208</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>2</td>\n",
       "      <td>1977</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "      <td>BRAZIL</td>\n",
       "      <td>BRAZIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5770290</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>2</td>\n",
       "      <td>1946</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>PANAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4052101</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>WT</td>\n",
       "      <td>NETHERLANDS</td>\n",
       "      <td>NETHERLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4273097</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>1958</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>WT</td>\n",
       "      <td>NETHERLANDS</td>\n",
       "      <td>NETHERLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5831892</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>2</td>\n",
       "      <td>1955</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   immigrant_id  country_1  country_2  visa_code  birth_year  \\\n",
       "0       5889208        689        689          2        1977   \n",
       "1       5770290        504        504          2        1946   \n",
       "2       4052101        123        123          2        2006   \n",
       "3       4273097        123        123          2        1958   \n",
       "4       5831892        582        582          2        1955   \n",
       "\n",
       "  non_inmigrant_sex ins_number visa_type  \\\n",
       "0                 M       None        B2   \n",
       "1                 F       None        B2   \n",
       "2                 M       None        WT   \n",
       "3                 M       None        WT   \n",
       "4                 F       None        B2   \n",
       "\n",
       "                                      country_1_name  \\\n",
       "0                                             BRAZIL   \n",
       "1                                             PANAMA   \n",
       "2                                        NETHERLANDS   \n",
       "3                                        NETHERLANDS   \n",
       "4    MEXICO Air Sea, and Not Reported (I-94, no l...   \n",
       "\n",
       "                                      country_2_name  \n",
       "0                                             BRAZIL  \n",
       "1                                             PANAMA  \n",
       "2                                        NETHERLANDS  \n",
       "3                                        NETHERLANDS  \n",
       "4    MEXICO Air Sea, and Not Reported (I-94, no l...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_personal_info.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_travel_data (df, output_data):\n",
    "    '''\n",
    "    Function which saves the dataframe already adapted to the data lake in Amazon S3.\n",
    "    \n",
    "    INPUT: \n",
    "    df - Spark Dataframe: data to be saved in Amazon S3\n",
    "    output_data - string: url to the Amazon S3 bucket\n",
    "    \n",
    "    OUTPUT:\n",
    "    None\n",
    "    '''\n",
    "    df.write.partitionBy('year').mode('overwrite').parquet(output_data + 'travel_data/' + datetime.datetime.strf(\"%Y%m%d\"))\n",
    "    return\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_personal_data (df, output_data):\n",
    "    '''\n",
    "    Function which saves the dataframe already adapted to the data lake in Amazon S3.\n",
    "    \n",
    "    INPUT: \n",
    "    df - Spark Dataframe: data to be saved in Amazon S3\n",
    "    output_data - string: url to the Amazon S3 bucket\n",
    "    \n",
    "    OUTPUT:\n",
    "    None\n",
    "    '''\n",
    "    df.write.partitionBy('country_1_name').mode('overwrite').parquet(output_data + 'personal_data/' + datetime.datetime.strf(\"%Y%m%d\"))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_travel_data(df_travel, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_personal_data(df_personal_info, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Airport Code Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the code structure of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ident         55075 non-null  object \n",
      " 1   type          55075 non-null  object \n",
      " 2   name          55075 non-null  object \n",
      " 3   elevation_ft  48069 non-null  float64\n",
      " 4   continent     27356 non-null  object \n",
      " 5   iso_country   54828 non-null  object \n",
      " 6   iso_region    55075 non-null  object \n",
      " 7   municipality  49399 non-null  object \n",
      " 8   gps_code      41030 non-null  object \n",
      " 9   iata_code     9189 non-null   object \n",
      " 10  local_code    28686 non-null  object \n",
      " 11  coordinates   55075 non-null  object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_airport.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>48069.000000</td>\n",
       "      <td>27356</td>\n",
       "      <td>54828</td>\n",
       "      <td>55075</td>\n",
       "      <td>49399</td>\n",
       "      <td>41030</td>\n",
       "      <td>9189</td>\n",
       "      <td>28686</td>\n",
       "      <td>55075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>55075</td>\n",
       "      <td>7</td>\n",
       "      <td>52144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>243</td>\n",
       "      <td>2810</td>\n",
       "      <td>27133</td>\n",
       "      <td>40850</td>\n",
       "      <td>9042</td>\n",
       "      <td>27436</td>\n",
       "      <td>54874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>4OK4</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Centre Hospitalier Heliport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EU</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>MBAC</td>\n",
       "      <td>0</td>\n",
       "      <td>AMA</td>\n",
       "      <td>0, 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>33965</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7840</td>\n",
       "      <td>22757</td>\n",
       "      <td>2277</td>\n",
       "      <td>404</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1240.789677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1602.363459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1266.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ident           type                         name  elevation_ft  \\\n",
       "count   55075          55075                        55075  48069.000000   \n",
       "unique  55075              7                        52144           NaN   \n",
       "top      4OK4  small_airport  Centre Hospitalier Heliport           NaN   \n",
       "freq        1          33965                           85           NaN   \n",
       "mean      NaN            NaN                          NaN   1240.789677   \n",
       "std       NaN            NaN                          NaN   1602.363459   \n",
       "min       NaN            NaN                          NaN  -1266.000000   \n",
       "25%       NaN            NaN                          NaN    205.000000   \n",
       "50%       NaN            NaN                          NaN    718.000000   \n",
       "75%       NaN            NaN                          NaN   1497.000000   \n",
       "max       NaN            NaN                          NaN  22000.000000   \n",
       "\n",
       "       continent iso_country iso_region municipality gps_code iata_code  \\\n",
       "count      27356       54828      55075        49399    41030      9189   \n",
       "unique         6         243       2810        27133    40850      9042   \n",
       "top           EU          US      US-TX        Seoul     MBAC         0   \n",
       "freq        7840       22757       2277          404        3        80   \n",
       "mean         NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "std          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "min          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "25%          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "50%          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "75%          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "max          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "\n",
       "       local_code coordinates  \n",
       "count       28686       55075  \n",
       "unique      27436       54874  \n",
       "top           AMA        0, 0  \n",
       "freq            5          53  \n",
       "mean          NaN         NaN  \n",
       "std           NaN         NaN  \n",
       "min           NaN         NaN  \n",
       "25%           NaN         NaN  \n",
       "50%           NaN         NaN  \n",
       "75%           NaN         NaN  \n",
       "max           NaN         NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variables with a high amount of missing values, so it is going to be necessary to analyze, if they have to be removed or those values can be filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent       27719\n",
       "iso_country       247\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident            0.000000\n",
       "type             0.000000\n",
       "name             0.000000\n",
       "elevation_ft    12.720835\n",
       "continent       50.329551\n",
       "iso_country      0.448479\n",
       "iso_region       0.000000\n",
       "municipality    10.305946\n",
       "gps_code        25.501589\n",
       "iata_code       83.315479\n",
       "local_code      47.914662\n",
       "coordinates      0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.isna().sum()/df_airport.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55065</th>\n",
       "      <td>ZYTH</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Tahe Airport</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Tahe</td>\n",
       "      <td>ZYTH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.720222222, 52.2244444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55071</th>\n",
       "      <td>ZYYY</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Shenyang Dongta Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Shenyang</td>\n",
       "      <td>ZYYY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.49600219726562, 41.784400939941406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55072</th>\n",
       "      <td>ZZ-0001</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Sealand Helipad</td>\n",
       "      <td>40.0</td>\n",
       "      <td>EU</td>\n",
       "      <td>GB</td>\n",
       "      <td>GB-ENG</td>\n",
       "      <td>Sealand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4825, 51.894444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55073</th>\n",
       "      <td>ZZ-0002</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Glorioso Islands Airstrip</td>\n",
       "      <td>11.0</td>\n",
       "      <td>AF</td>\n",
       "      <td>TF</td>\n",
       "      <td>TF-U-A</td>\n",
       "      <td>Grande Glorieuse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.296388888900005, -11.584277777799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55074</th>\n",
       "      <td>ZZZZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Satsuma IÅjima Airport</td>\n",
       "      <td>338.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>JP</td>\n",
       "      <td>JP-46</td>\n",
       "      <td>Mishima-Mura</td>\n",
       "      <td>RJX7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.270556, 30.784722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45886 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ident            type                                name  \\\n",
       "0          00A        heliport                   Total Rf Heliport   \n",
       "1         00AA   small_airport                Aero B Ranch Airport   \n",
       "2         00AK   small_airport                        Lowell Field   \n",
       "3         00AL   small_airport                        Epps Airpark   \n",
       "4         00AR          closed  Newport Hospital & Clinic Heliport   \n",
       "...        ...             ...                                 ...   \n",
       "55065     ZYTH   small_airport                        Tahe Airport   \n",
       "55071     ZYYY  medium_airport             Shenyang Dongta Airport   \n",
       "55072  ZZ-0001        heliport                     Sealand Helipad   \n",
       "55073  ZZ-0002   small_airport           Glorioso Islands Airstrip   \n",
       "55074     ZZZZ   small_airport             Satsuma IÅjima Airport   \n",
       "\n",
       "       elevation_ft continent iso_country iso_region      municipality  \\\n",
       "0              11.0       NaN          US      US-PA          Bensalem   \n",
       "1            3435.0       NaN          US      US-KS             Leoti   \n",
       "2             450.0       NaN          US      US-AK      Anchor Point   \n",
       "3             820.0       NaN          US      US-AL           Harvest   \n",
       "4             237.0       NaN          US      US-AR           Newport   \n",
       "...             ...       ...         ...        ...               ...   \n",
       "55065        1240.0        AS          CN      CN-23              Tahe   \n",
       "55071           NaN        AS          CN      CN-21          Shenyang   \n",
       "55072          40.0        EU          GB     GB-ENG           Sealand   \n",
       "55073          11.0        AF          TF     TF-U-A  Grande Glorieuse   \n",
       "55074         338.0        AS          JP      JP-46      Mishima-Mura   \n",
       "\n",
       "      gps_code iata_code local_code                              coordinates  \n",
       "0          00A       NaN        00A       -74.93360137939453, 40.07080078125  \n",
       "1         00AA       NaN       00AA                   -101.473911, 38.704022  \n",
       "2         00AK       NaN       00AK              -151.695999146, 59.94919968  \n",
       "3         00AL       NaN       00AL    -86.77030181884766, 34.86479949951172  \n",
       "4          NaN       NaN        NaN                      -91.254898, 35.6087  \n",
       "...        ...       ...        ...                                      ...  \n",
       "55065     ZYTH       NaN        NaN             124.720222222, 52.2244444444  \n",
       "55071     ZYYY       NaN        NaN   123.49600219726562, 41.784400939941406  \n",
       "55072      NaN       NaN        NaN                        1.4825, 51.894444  \n",
       "55073      NaN       NaN        NaN  47.296388888900005, -11.584277777799999  \n",
       "55074     RJX7       NaN        NaN                    130.270556, 30.784722  \n",
       "\n",
       "[45886 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport[df_airport.iata_code.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ident, type, name, elevation_ft, continent, iso_country, iso_region, municipality, gps_code, iata_code, local_code, coordinates]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport[df_airport.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_airport_data(df_airport):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Modify the names of the columns to more descriptive values\n",
    "    2. Drop unnecessary columns\n",
    "    3. Modify data types\n",
    "    4. Replace codes with more descriptive values\n",
    "    5. Drop duplicated values\n",
    "    6. Create new features\n",
    "    \n",
    "    INPUT:\n",
    "    df_airport (Spark DataFrame): DataFrame with\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_airport (Spark DataFrame): data already cleaned for being uploaded to the data lake\n",
    "    '''\n",
    "    \n",
    "    # 1. Modify the names of the columns to more descriptive values\n",
    "    airport_coordinates = df_airport['coordinates'].str.split(',', expand = True).rename(columns={0:'latitude', 1:'longitude'})\n",
    "    df_airport['latitude'] = airport_coordinates['latitude']\n",
    "    df_airport['longitude'] = airport_coordinates['longitude']\n",
    "    df_airport.drop('coordinates', axis = 1, inplace = True)\n",
    "    \n",
    "    # 2. Drop unnecessary columns\n",
    "    df_airport.drop(['iata_code','continent', 'gps_code', 'local_code', 'elevation_ft', 'municipality',\n",
    "                 'iso_country'], axis = 1, inplace = True)\n",
    "    \n",
    "    # 3. Modify data types\n",
    "    df_airport.latitude = df_airport.latitude.astype('float').round(2)\n",
    "    df_airport.longitude = df_airport.longitude.astype('float').round(2)\n",
    "    df_iso_states = pd.read_csv('ISO_code_US.csv', sep=\";\")\n",
    "\n",
    "    # 4. Replace codes with more descriptive values\n",
    "    df_iso_states = pd.read_csv('ISO_code_US.csv', sep=\";\")\n",
    "    df_airport = pd.merge(df_airport, df_iso_states, how = 'inner', left_on=['iso_region'], right_on = ['iso_code'])\n",
    "    df_airport.drop(\"iso_code\", axis = 1, inplace = True)\n",
    "    \n",
    "    # 5. Drop duplicated values\n",
    "    df_airport = df_airport.drop_duplicates()\n",
    "    \n",
    "    # 6. Create new features\n",
    "    # 6.1. city_state - Key to link this table with df_sas\n",
    "    df_faa_code = pd.read_csv('faa_code.csv', sep = ';') # Dataframe with the FAA codes to \n",
    "    df_faa_code['City'] = df_faa_code['City'].str.capitalize()\n",
    "    df_faa_code['State'] = df_faa_code['State'].str.capitalize()\n",
    "    df_faa_code['city_state'] = df_faa_code.City + '_' + df_faa_code.State\n",
    "    df_faa_code.drop_duplicates(subset = ['Locator Id'], inplace = True)\n",
    "    df_airport = pd.merge(df_airport, df_faa_code, how = 'inner', left_on=['ident'], right_on = ['Locator Id'])\n",
    "    df_airport.drop(['state', 'Locator Id', 'Facilty'], axis = 1, inplace = True)\n",
    "\n",
    "    return df_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>city_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-74.93</td>\n",
       "      <td>40.07</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Bensalem_Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00PN</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ferrell Field</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-80.21</td>\n",
       "      <td>41.30</td>\n",
       "      <td>Mercer</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Mercer_Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01PA</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Pine Heliport</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-80.05</td>\n",
       "      <td>40.66</td>\n",
       "      <td>Bolivar</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Bolivar_Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01PS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Nort's Resort Airport</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-76.03</td>\n",
       "      <td>41.60</td>\n",
       "      <td>Meshoppen</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Meshoppen_Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02PA</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Lag Iii Heliport</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-79.77</td>\n",
       "      <td>40.44</td>\n",
       "      <td>Monroeville</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Monroeville_Pennsylvania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                   name iso_region  latitude  longitude  \\\n",
       "0   00A       heliport      Total Rf Heliport      US-PA    -74.93      40.07   \n",
       "1  00PN  small_airport          Ferrell Field      US-PA    -80.21      41.30   \n",
       "2  01PA       heliport          Pine Heliport      US-PA    -80.05      40.66   \n",
       "3  01PS  small_airport  Nort's Resort Airport      US-PA    -76.03      41.60   \n",
       "4  02PA       heliport       Lag Iii Heliport      US-PA    -79.77      40.44   \n",
       "\n",
       "          City         State                city_state  \n",
       "0     Bensalem  Pennsylvania     Bensalem_Pennsylvania  \n",
       "1       Mercer  Pennsylvania       Mercer_Pennsylvania  \n",
       "2      Bolivar  Pennsylvania      Bolivar_Pennsylvania  \n",
       "3    Meshoppen  Pennsylvania    Meshoppen_Pennsylvania  \n",
       "4  Monroeville  Pennsylvania  Monroeville_Pennsylvania  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport = cleaning_airport_data(df_airport)\n",
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_S3 (df, output_data = output_data):\n",
    "    '''\n",
    "    Function which saves the dataframe already adapted to the data lake in Amazon S3.\n",
    "    \n",
    "    INPUT: \n",
    "    df_airport - Spark Dataframe: data to be saved in Amazon S3\n",
    "    output_data - string: url to the Amazon S3 bucket\n",
    "    \n",
    "    OUTPUT:\n",
    "    None\n",
    "    '''\n",
    "    df.to_csv(output_data + 'immigration/')\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_airport_data(df_airport, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. U.S. Cities Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   City                    2891 non-null   object \n",
      " 1   State                   2891 non-null   object \n",
      " 2   Median Age              2891 non-null   float64\n",
      " 3   Male Population         2888 non-null   float64\n",
      " 4   Female Population       2888 non-null   float64\n",
      " 5   Total Population        2891 non-null   int64  \n",
      " 6   Number of Veterans      2878 non-null   float64\n",
      " 7   Foreign-born            2878 non-null   float64\n",
      " 8   Average Household Size  2875 non-null   float64\n",
      " 9   State Code              2891 non-null   object \n",
      " 10  Race                    2891 non-null   object \n",
      " 11  Count                   2891 non-null   int64  \n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cities.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2891</td>\n",
       "      <td>2891</td>\n",
       "      <td>2891.000000</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2878.000000</td>\n",
       "      <td>2.878000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2891</td>\n",
       "      <td>2891</td>\n",
       "      <td>2.891000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>567</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>California</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>676</td>\n",
       "      <td>596</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.494881</td>\n",
       "      <td>9.732843e+04</td>\n",
       "      <td>1.017696e+05</td>\n",
       "      <td>1.989668e+05</td>\n",
       "      <td>9367.832523</td>\n",
       "      <td>4.065360e+04</td>\n",
       "      <td>2.742543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.896377e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.401617</td>\n",
       "      <td>2.162999e+05</td>\n",
       "      <td>2.315646e+05</td>\n",
       "      <td>4.475559e+05</td>\n",
       "      <td>13211.219924</td>\n",
       "      <td>1.557491e+05</td>\n",
       "      <td>0.433291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.443856e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>2.928100e+04</td>\n",
       "      <td>2.734800e+04</td>\n",
       "      <td>6.321500e+04</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>8.610000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.800000</td>\n",
       "      <td>3.928900e+04</td>\n",
       "      <td>4.122700e+04</td>\n",
       "      <td>8.042900e+04</td>\n",
       "      <td>3739.000000</td>\n",
       "      <td>9.224000e+03</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.435000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>5.234100e+04</td>\n",
       "      <td>5.380900e+04</td>\n",
       "      <td>1.067820e+05</td>\n",
       "      <td>5397.000000</td>\n",
       "      <td>1.882200e+04</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.378000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.664175e+04</td>\n",
       "      <td>8.960400e+04</td>\n",
       "      <td>1.752320e+05</td>\n",
       "      <td>9368.000000</td>\n",
       "      <td>3.397175e+04</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.444700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>4.081698e+06</td>\n",
       "      <td>4.468707e+06</td>\n",
       "      <td>8.550405e+06</td>\n",
       "      <td>156961.000000</td>\n",
       "      <td>3.212500e+06</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.835726e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City       State   Median Age  Male Population  Female Population  \\\n",
       "count       2891        2891  2891.000000     2.888000e+03       2.888000e+03   \n",
       "unique       567          49          NaN              NaN                NaN   \n",
       "top     Columbia  California          NaN              NaN                NaN   \n",
       "freq          15         676          NaN              NaN                NaN   \n",
       "mean         NaN         NaN    35.494881     9.732843e+04       1.017696e+05   \n",
       "std          NaN         NaN     4.401617     2.162999e+05       2.315646e+05   \n",
       "min          NaN         NaN    22.900000     2.928100e+04       2.734800e+04   \n",
       "25%          NaN         NaN    32.800000     3.928900e+04       4.122700e+04   \n",
       "50%          NaN         NaN    35.300000     5.234100e+04       5.380900e+04   \n",
       "75%          NaN         NaN    38.000000     8.664175e+04       8.960400e+04   \n",
       "max          NaN         NaN    70.500000     4.081698e+06       4.468707e+06   \n",
       "\n",
       "        Total Population  Number of Veterans  Foreign-born  \\\n",
       "count       2.891000e+03         2878.000000  2.878000e+03   \n",
       "unique               NaN                 NaN           NaN   \n",
       "top                  NaN                 NaN           NaN   \n",
       "freq                 NaN                 NaN           NaN   \n",
       "mean        1.989668e+05         9367.832523  4.065360e+04   \n",
       "std         4.475559e+05        13211.219924  1.557491e+05   \n",
       "min         6.321500e+04          416.000000  8.610000e+02   \n",
       "25%         8.042900e+04         3739.000000  9.224000e+03   \n",
       "50%         1.067820e+05         5397.000000  1.882200e+04   \n",
       "75%         1.752320e+05         9368.000000  3.397175e+04   \n",
       "max         8.550405e+06       156961.000000  3.212500e+06   \n",
       "\n",
       "        Average Household Size State Code                Race         Count  \n",
       "count              2875.000000       2891                2891  2.891000e+03  \n",
       "unique                     NaN         49                   5           NaN  \n",
       "top                        NaN         CA  Hispanic or Latino           NaN  \n",
       "freq                       NaN        676                 596           NaN  \n",
       "mean                  2.742543        NaN                 NaN  4.896377e+04  \n",
       "std                   0.433291        NaN                 NaN  1.443856e+05  \n",
       "min                   2.000000        NaN                 NaN  9.800000e+01  \n",
       "25%                   2.430000        NaN                 NaN  3.435000e+03  \n",
       "50%                   2.650000        NaN                 NaN  1.378000e+04  \n",
       "75%                   2.950000        NaN                 NaN  5.444700e+04  \n",
       "max                   4.980000        NaN                 NaN  3.835726e+06  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>335559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>76349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>12143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>139967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>4031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Ponce</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.5</td>\n",
       "      <td>56968.0</td>\n",
       "      <td>64615.0</td>\n",
       "      <td>121583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>120705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>Bayamón</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>39.4</td>\n",
       "      <td>80128.0</td>\n",
       "      <td>90131.0</td>\n",
       "      <td>170259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>169155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>69936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>65521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          City        State  Median Age  Male Population  Female Population  \\\n",
       "111   San Juan  Puerto Rico        41.4         155408.0           186829.0   \n",
       "155     Caguas  Puerto Rico        40.4          34743.0            42265.0   \n",
       "258   Carolina  Puerto Rico        42.0          64758.0            77308.0   \n",
       "637   Carolina  Puerto Rico        42.0          64758.0            77308.0   \n",
       "1747  San Juan  Puerto Rico        41.4         155408.0           186829.0   \n",
       "1748  Mayagüez  Puerto Rico        38.1          30799.0            35782.0   \n",
       "1995     Ponce  Puerto Rico        40.5          56968.0            64615.0   \n",
       "2004   Bayamón  Puerto Rico        39.4          80128.0            90131.0   \n",
       "2441  San Juan  Puerto Rico        41.4         155408.0           186829.0   \n",
       "2589  Guaynabo  Puerto Rico        42.2          33066.0            37426.0   \n",
       "2597    Caguas  Puerto Rico        40.4          34743.0            42265.0   \n",
       "2654  Guaynabo  Puerto Rico        42.2          33066.0            37426.0   \n",
       "2746  Mayagüez  Puerto Rico        38.1          30799.0            35782.0   \n",
       "\n",
       "      Total Population  Number of Veterans  Foreign-born  \\\n",
       "111             342237                 NaN           NaN   \n",
       "155              77008                 NaN           NaN   \n",
       "258             142066                 NaN           NaN   \n",
       "637             142066                 NaN           NaN   \n",
       "1747            342237                 NaN           NaN   \n",
       "1748             66581                 NaN           NaN   \n",
       "1995            121583                 NaN           NaN   \n",
       "2004            170259                 NaN           NaN   \n",
       "2441            342237                 NaN           NaN   \n",
       "2589             70492                 NaN           NaN   \n",
       "2597             77008                 NaN           NaN   \n",
       "2654             70492                 NaN           NaN   \n",
       "2746             66581                 NaN           NaN   \n",
       "\n",
       "      Average Household Size State Code                               Race  \\\n",
       "111                      NaN         PR                 Hispanic or Latino   \n",
       "155                      NaN         PR                 Hispanic or Latino   \n",
       "258                      NaN         PR  American Indian and Alaska Native   \n",
       "637                      NaN         PR                 Hispanic or Latino   \n",
       "1747                     NaN         PR  American Indian and Alaska Native   \n",
       "1748                     NaN         PR                              Asian   \n",
       "1995                     NaN         PR                 Hispanic or Latino   \n",
       "2004                     NaN         PR                 Hispanic or Latino   \n",
       "2441                     NaN         PR                              Asian   \n",
       "2589                     NaN         PR                 Hispanic or Latino   \n",
       "2597                     NaN         PR  American Indian and Alaska Native   \n",
       "2654                     NaN         PR  American Indian and Alaska Native   \n",
       "2746                     NaN         PR                 Hispanic or Latino   \n",
       "\n",
       "       Count  \n",
       "111   335559  \n",
       "155    76349  \n",
       "258    12143  \n",
       "637   139967  \n",
       "1747    4031  \n",
       "1748     235  \n",
       "1995  120705  \n",
       "2004  169155  \n",
       "2441    2452  \n",
       "2589   69936  \n",
       "2597     624  \n",
       "2654     589  \n",
       "2746   65521  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities[df_cities['Number of Veterans'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>335559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>76349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>12143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>The Villages</td>\n",
       "      <td>Florida</td>\n",
       "      <td>70.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72590</td>\n",
       "      <td>15231.0</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>The Villages</td>\n",
       "      <td>Florida</td>\n",
       "      <td>70.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72590</td>\n",
       "      <td>15231.0</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>139967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>The Villages</td>\n",
       "      <td>Florida</td>\n",
       "      <td>70.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72590</td>\n",
       "      <td>15231.0</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>White</td>\n",
       "      <td>72211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>4031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Ponce</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.5</td>\n",
       "      <td>56968.0</td>\n",
       "      <td>64615.0</td>\n",
       "      <td>121583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>120705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>Bayamón</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>39.4</td>\n",
       "      <td>80128.0</td>\n",
       "      <td>90131.0</td>\n",
       "      <td>170259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>169155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>69936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>65521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              City        State  Median Age  Male Population  \\\n",
       "111       San Juan  Puerto Rico        41.4         155408.0   \n",
       "155         Caguas  Puerto Rico        40.4          34743.0   \n",
       "258       Carolina  Puerto Rico        42.0          64758.0   \n",
       "333   The Villages      Florida        70.5              NaN   \n",
       "449   The Villages      Florida        70.5              NaN   \n",
       "637       Carolina  Puerto Rico        42.0          64758.0   \n",
       "1437  The Villages      Florida        70.5              NaN   \n",
       "1747      San Juan  Puerto Rico        41.4         155408.0   \n",
       "1748      Mayagüez  Puerto Rico        38.1          30799.0   \n",
       "1995         Ponce  Puerto Rico        40.5          56968.0   \n",
       "2004       Bayamón  Puerto Rico        39.4          80128.0   \n",
       "2441      San Juan  Puerto Rico        41.4         155408.0   \n",
       "2589      Guaynabo  Puerto Rico        42.2          33066.0   \n",
       "2597        Caguas  Puerto Rico        40.4          34743.0   \n",
       "2654      Guaynabo  Puerto Rico        42.2          33066.0   \n",
       "2746      Mayagüez  Puerto Rico        38.1          30799.0   \n",
       "\n",
       "      Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "111            186829.0            342237                 NaN           NaN   \n",
       "155             42265.0             77008                 NaN           NaN   \n",
       "258             77308.0            142066                 NaN           NaN   \n",
       "333                 NaN             72590             15231.0        4034.0   \n",
       "449                 NaN             72590             15231.0        4034.0   \n",
       "637             77308.0            142066                 NaN           NaN   \n",
       "1437                NaN             72590             15231.0        4034.0   \n",
       "1747           186829.0            342237                 NaN           NaN   \n",
       "1748            35782.0             66581                 NaN           NaN   \n",
       "1995            64615.0            121583                 NaN           NaN   \n",
       "2004            90131.0            170259                 NaN           NaN   \n",
       "2441           186829.0            342237                 NaN           NaN   \n",
       "2589            37426.0             70492                 NaN           NaN   \n",
       "2597            42265.0             77008                 NaN           NaN   \n",
       "2654            37426.0             70492                 NaN           NaN   \n",
       "2746            35782.0             66581                 NaN           NaN   \n",
       "\n",
       "      Average Household Size State Code                               Race  \\\n",
       "111                      NaN         PR                 Hispanic or Latino   \n",
       "155                      NaN         PR                 Hispanic or Latino   \n",
       "258                      NaN         PR  American Indian and Alaska Native   \n",
       "333                      NaN         FL                 Hispanic or Latino   \n",
       "449                      NaN         FL          Black or African-American   \n",
       "637                      NaN         PR                 Hispanic or Latino   \n",
       "1437                     NaN         FL                              White   \n",
       "1747                     NaN         PR  American Indian and Alaska Native   \n",
       "1748                     NaN         PR                              Asian   \n",
       "1995                     NaN         PR                 Hispanic or Latino   \n",
       "2004                     NaN         PR                 Hispanic or Latino   \n",
       "2441                     NaN         PR                              Asian   \n",
       "2589                     NaN         PR                 Hispanic or Latino   \n",
       "2597                     NaN         PR  American Indian and Alaska Native   \n",
       "2654                     NaN         PR  American Indian and Alaska Native   \n",
       "2746                     NaN         PR                 Hispanic or Latino   \n",
       "\n",
       "       Count  \n",
       "111   335559  \n",
       "155    76349  \n",
       "258    12143  \n",
       "333     1066  \n",
       "449      331  \n",
       "637   139967  \n",
       "1437   72211  \n",
       "1747    4031  \n",
       "1748     235  \n",
       "1995  120705  \n",
       "2004  169155  \n",
       "2441    2452  \n",
       "2589   69936  \n",
       "2597     624  \n",
       "2654     589  \n",
       "2746   65521  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities[df_cities['Average Household Size'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities[df_cities.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cities_data(df_cities):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Modify the names of the columns to more descriptive values\n",
    "    2. Fix the missing values issues\n",
    "    3. Modify data types\n",
    "    4. Create new features\n",
    "    \n",
    "    INPUT:\n",
    "    df_cities (Spark DataFrame): DataFrame directly extracted from the raw data without any cleaning process\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_cities (Spark DataFrame): data already cleaned for being uploaded to the data lake\n",
    "    '''\n",
    "    \n",
    "    # 1. Modify the names of the columns to more descriptive values\n",
    "    df_cities.columns = df_cities.columns.str.lower()\n",
    "    df_cities.columns = df_cities.columns.str.replace(\" \",\"_\")\n",
    "    df_cities.columns = df_cities.columns.str.replace(\"foreign-born\",\"foreign_born\")\n",
    "    \n",
    "    # 2. Fix the missing values issues\n",
    "    # median age\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['median_age'].mean()>0:\n",
    "            df_cities['median_age'] = df_cities.groupby('state')['median_age'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['median_age'] = df_cities.groupby('state')['median_age'].transform(lambda x: x.fillna(df_cities.median_age.mean()))\n",
    "            \n",
    "    # male population\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['male_population'].mean()>0:\n",
    "            df_cities['male_population'] = df_cities.groupby('state')['male_population'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities.male_population = df_cities.groupby('state')['male_population'].transform(lambda x: x.fillna(df_cities.male_population.mean()))\n",
    "            \n",
    "    # female population\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['female_population'].mean()>0:\n",
    "            df_cities['female_population'] = df_cities.groupby('state')['female_population'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities.male_population = df_cities.groupby('state')['female_population'].transform(lambda x: x.fillna(df_cities.female_population.mean()))\n",
    "    \n",
    "    # total population\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['total_population'].mean()>0:\n",
    "            df_cities['total_population'] = df_cities.groupby('state')['total_population'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['total_population'] = df_cities.groupby('state')['total_population'].transform(lambda x: x.fillna(df_cities.total_population.mean()))    \n",
    "            \n",
    "    # number of veterans\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['number_of_veterans'].mean()>0:\n",
    "            df_cities['number_of_veterans'] = df_cities.groupby('state')['number_of_veterans'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['number_of_veterans'] = df_cities.groupby('state')['number_of_veterans'].transform(lambda x: x.fillna(df_cities.number_of_veterans.mean()))\n",
    "    \n",
    "    # foreign born\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['foreign_born'].mean()>0:\n",
    "            df_cities['foreign_born'] = df_cities.groupby('state')['foreign_born'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['foreign_born'] = df_cities.groupby('state')['foreign_born'].transform(lambda x: x.fillna(df_cities.foreign_born.mean()))\n",
    "    \n",
    "    # average household size\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['average_household_size'].mean()>0:\n",
    "            df_cities['average_household_size'] = df_cities.groupby('state')['average_household_size'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities.average_household_size = df_cities.groupby('state')['average_household_size'].transform(lambda x: x.fillna(df_cities.average_household_size.mean()))\n",
    "    \n",
    "    # count\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['count'].mean()>0:\n",
    "            df_cities['count'] = df_cities.groupby('state')['count'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['count'] = df_cities.groupby('state')['count'].transform(lambda x: x.fillna(df_cities.count.mean()))\n",
    "    \n",
    "    # Drop the lines of the columns which have key values that can not be infered\n",
    "    df_cities.dropna(subset = ['city','state','state_code','race'], inplace = True)\n",
    "    \n",
    "    # 3. Modify data types\n",
    "    list_float_to_int = ['male_population','female_population','total_population','number_of_veterans',\n",
    "                    'foreign_born','count']\n",
    "\n",
    "    for col in list_float_to_int:\n",
    "        df_cities[col] = df_cities[col].astype('int')\n",
    "    \n",
    "    # 4. Create new features\n",
    "    # 4.1. city_state - Variable to link df_cities with df_travel\n",
    "    var_names_list = ['median_age', 'male_population', 'female_population',\n",
    "           'total_population', 'number_of_veterans', 'foreign_born',\n",
    "           'average_household_size', 'count']\n",
    "\n",
    "    race_list = ['Hispanic or Latino','White','Black or African-American','Asian','American Indian and Alaska Native']\n",
    "\n",
    "    for var in var_names_list:\n",
    "        for race_var in race_list:\n",
    "            name_var = var + '_' + race_var\n",
    "            df_cities[name_var] = df_cities[df_cities['race'] == race_var][var]\n",
    "    df_cities.head()\n",
    "\n",
    "    df_cities.drop(['median_age', 'male_population', 'female_population','total_population', 'number_of_veterans', 'foreign_born',\n",
    "           'average_household_size', 'count','race'], axis = 1, inplace = True)\n",
    "\n",
    "    df_cities['city_state'] = df_cities['city'] + '_' + df_cities['state']\n",
    "    df_cities = df_cities.groupby('city_state').sum().reset_index()\n",
    "\n",
    "    df_cities[['city','state']] = df_cities.city_state.str.split('_', expand = True).rename(columns = {0:'city', 1:'state'})\n",
    "    \n",
    "    return df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_state</th>\n",
       "      <th>median_age_Hispanic or Latino</th>\n",
       "      <th>median_age_White</th>\n",
       "      <th>median_age_Black or African-American</th>\n",
       "      <th>median_age_Asian</th>\n",
       "      <th>median_age_American Indian and Alaska Native</th>\n",
       "      <th>male_population_Hispanic or Latino</th>\n",
       "      <th>male_population_White</th>\n",
       "      <th>male_population_Black or African-American</th>\n",
       "      <th>male_population_Asian</th>\n",
       "      <th>...</th>\n",
       "      <th>average_household_size_Black or African-American</th>\n",
       "      <th>average_household_size_Asian</th>\n",
       "      <th>average_household_size_American Indian and Alaska Native</th>\n",
       "      <th>count_Hispanic or Latino</th>\n",
       "      <th>count_White</th>\n",
       "      <th>count_Black or African-American</th>\n",
       "      <th>count_Asian</th>\n",
       "      <th>count_American Indian and Alaska Native</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene_Texas</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>65212.0</td>\n",
       "      <td>65212.0</td>\n",
       "      <td>65212.0</td>\n",
       "      <td>65212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.64</td>\n",
       "      <td>33222.0</td>\n",
       "      <td>95487.0</td>\n",
       "      <td>14449.0</td>\n",
       "      <td>2929.0</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Akron_Ohio</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>96886.0</td>\n",
       "      <td>96886.0</td>\n",
       "      <td>96886.0</td>\n",
       "      <td>96886.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3684.0</td>\n",
       "      <td>129192.0</td>\n",
       "      <td>66551.0</td>\n",
       "      <td>9033.0</td>\n",
       "      <td>1845.0</td>\n",
       "      <td>Akron</td>\n",
       "      <td>Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alafaya_Florida</td>\n",
       "      <td>33.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39504.0</td>\n",
       "      <td>39504.0</td>\n",
       "      <td>39504.0</td>\n",
       "      <td>39504.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34897.0</td>\n",
       "      <td>63666.0</td>\n",
       "      <td>6577.0</td>\n",
       "      <td>10336.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Alafaya</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alameda_California</td>\n",
       "      <td>41.4</td>\n",
       "      <td>41.4</td>\n",
       "      <td>41.4</td>\n",
       "      <td>41.4</td>\n",
       "      <td>41.4</td>\n",
       "      <td>37747.0</td>\n",
       "      <td>37747.0</td>\n",
       "      <td>37747.0</td>\n",
       "      <td>37747.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.52</td>\n",
       "      <td>8265.0</td>\n",
       "      <td>44232.0</td>\n",
       "      <td>7364.0</td>\n",
       "      <td>27984.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany_Georgia</td>\n",
       "      <td>33.3</td>\n",
       "      <td>33.3</td>\n",
       "      <td>33.3</td>\n",
       "      <td>33.3</td>\n",
       "      <td>33.3</td>\n",
       "      <td>31695.0</td>\n",
       "      <td>31695.0</td>\n",
       "      <td>31695.0</td>\n",
       "      <td>31695.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1783.0</td>\n",
       "      <td>17160.0</td>\n",
       "      <td>53440.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           city_state  median_age_Hispanic or Latino  median_age_White  \\\n",
       "0       Abilene_Texas                           31.3              31.3   \n",
       "1          Akron_Ohio                           38.1              38.1   \n",
       "2     Alafaya_Florida                           33.5              33.5   \n",
       "3  Alameda_California                           41.4              41.4   \n",
       "4      Albany_Georgia                           33.3              33.3   \n",
       "\n",
       "   median_age_Black or African-American  median_age_Asian  \\\n",
       "0                                  31.3              31.3   \n",
       "1                                  38.1              38.1   \n",
       "2                                  33.5              33.5   \n",
       "3                                  41.4              41.4   \n",
       "4                                  33.3              33.3   \n",
       "\n",
       "   median_age_American Indian and Alaska Native  \\\n",
       "0                                          31.3   \n",
       "1                                          38.1   \n",
       "2                                           0.0   \n",
       "3                                          41.4   \n",
       "4                                          33.3   \n",
       "\n",
       "   male_population_Hispanic or Latino  male_population_White  \\\n",
       "0                             65212.0                65212.0   \n",
       "1                             96886.0                96886.0   \n",
       "2                             39504.0                39504.0   \n",
       "3                             37747.0                37747.0   \n",
       "4                             31695.0                31695.0   \n",
       "\n",
       "   male_population_Black or African-American  male_population_Asian  ...  \\\n",
       "0                                    65212.0                65212.0  ...   \n",
       "1                                    96886.0                96886.0  ...   \n",
       "2                                    39504.0                39504.0  ...   \n",
       "3                                    37747.0                37747.0  ...   \n",
       "4                                    31695.0                31695.0  ...   \n",
       "\n",
       "   average_household_size_Black or African-American  \\\n",
       "0                                              2.64   \n",
       "1                                              2.24   \n",
       "2                                              2.94   \n",
       "3                                              2.52   \n",
       "4                                              2.38   \n",
       "\n",
       "   average_household_size_Asian  \\\n",
       "0                          2.64   \n",
       "1                          2.24   \n",
       "2                          2.94   \n",
       "3                          2.52   \n",
       "4                          2.38   \n",
       "\n",
       "   average_household_size_American Indian and Alaska Native  \\\n",
       "0                                               2.64          \n",
       "1                                               2.24          \n",
       "2                                               0.00          \n",
       "3                                               2.52          \n",
       "4                                               2.38          \n",
       "\n",
       "   count_Hispanic or Latino  count_White  count_Black or African-American  \\\n",
       "0                   33222.0      95487.0                          14449.0   \n",
       "1                    3684.0     129192.0                          66551.0   \n",
       "2                   34897.0      63666.0                           6577.0   \n",
       "3                    8265.0      44232.0                           7364.0   \n",
       "4                    1783.0      17160.0                          53440.0   \n",
       "\n",
       "   count_Asian  count_American Indian and Alaska Native     city       state  \n",
       "0       2929.0                                   1813.0  Abilene       Texas  \n",
       "1       9033.0                                   1845.0    Akron        Ohio  \n",
       "2      10336.0                                      0.0  Alafaya     Florida  \n",
       "3      27984.0                                   1329.0  Alameda  California  \n",
       "4        650.0                                    445.0   Albany     Georgia  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities = cleaning_cities_data(df_cities)\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_S3 (df, output_data = output_data):\n",
    "    '''\n",
    "    Function which saves the dataframe already adapted to the data lake in Amazon S3.\n",
    "    \n",
    "    INPUT: \n",
    "    df_airport - Spark Dataframe: data to be saved in Amazon S3\n",
    "    output_data - string: url to the Amazon S3 bucket\n",
    "    \n",
    "    OUTPUT:\n",
    "    None\n",
    "    '''\n",
    "    df.to_csv(output_data + 'immigration/')\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv_S3(df_cities, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. World Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_usa = df_temp.where(df_temp.Country == \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|1820-01-01 00:00:00|2.1010000000000004|                        3.217|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-02-01 00:00:00|             6.926|                        2.853|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-03-01 00:00:00|            10.767|                        2.395|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-04-01 00:00:00|17.988999999999994|                        2.202|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-05-01 00:00:00|            21.809|                        2.036|Abilene|United States|  32.95N|  100.53W|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_usa.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_usa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|summary|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|Latitude|Longitude|\n",
      "+-------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|  count|            661524|                       661524| 687289|       687289|  687289|   687289|\n",
      "|   mean|13.949334923600677|           1.0895499452778745|   null|         null|    null|     null|\n",
      "| stddev| 9.173337261791232|           1.1506804949928662|   null|         null|    null|     null|\n",
      "|    min|           -25.163|                         0.04|Abilene|United States|  26.52N|  100.53W|\n",
      "|    max|            34.379|                       10.519|Yonkers|United States|  61.88N|   99.24W|\n",
      "+-------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_usa.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_temperature_data(df_temp):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Modify the data types\n",
    "    2. Fix the missing values issues\n",
    "    3. Drop unnecessary columns\n",
    "    \n",
    "    INPUT:\n",
    "    df_temp (Spark DataFrame): DataFrame directly extracted from the raw data without any cleaning process\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_temp_usa (Spark DataFrame): data already cleaned for being uploaded to the data lake\n",
    "    '''\n",
    "    df_temp_usa = df_temp.where(df_temp.Country == \"United States\")\n",
    "    \n",
    "    # 1. Modify the data types\n",
    "    # Change the datastamp format, since the hour, minutes and seconds are not necessary and do not contribute to the final result\n",
    "    df_temp_usa = df_temp_usa.withColumn('dt', to_date('dt'))\n",
    "    \n",
    "    # 2. Fix the missing values issues\n",
    "    df_pandas = df_temp_usa.toPandas()\n",
    "    df_pandas['AverageTemperature'] = df_pandas.groupby('City')['AverageTemperature'].transform(lambda x: x.fillna(x.mean()))\n",
    "    df_pandas['AverageTemperatureUncertainty'] = df_pandas.groupby('City')['AverageTemperatureUncertainty'].transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # 3. Drop unnecessary columns\n",
    "    df_pandas.drop(\"Country\", axis = 1, inplace = True)\n",
    "    \n",
    "    return df_temp_usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_usa = cleaning_temperature_data(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "0  1820-01-01               2.101                          3.217  Abilene   \n",
       "1  1820-02-01               6.926                          2.853  Abilene   \n",
       "2  1820-03-01              10.767                          2.395  Abilene   \n",
       "3  1820-04-01              17.989                          2.202  Abilene   \n",
       "4  1820-05-01              21.809                          2.036  Abilene   \n",
       "\n",
       "         Country Latitude Longitude  \n",
       "0  United States   32.95N   100.53W  \n",
       "1  United States   32.95N   100.53W  \n",
       "2  United States   32.95N   100.53W  \n",
       "3  United States   32.95N   100.53W  \n",
       "4  United States   32.95N   100.53W  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_usa.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_temp_usa_data (df_temp_usa = df_temp_usa, output_data = output_data):\n",
    "    '''\n",
    "    Function which saves the dataframe already adapted to the data lake in Amazon S3.\n",
    "    \n",
    "    INPUT: \n",
    "    df_airport - Spark Dataframe: data to be saved in Amazon S3\n",
    "    output_data - string: url to the Amazon S3 bucket\n",
    "    \n",
    "    OUTPUT:\n",
    "    None\n",
    "    '''\n",
    "    df_temp_usa.write.partitionBy('dt').mode('overwrite').parquet(output_data + 'temp_eeuu/' + datetime.datetime.strf(\"%Y%m%d\"))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_temp_usa_data(df_temp_eeuu, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Examples of analysis\n",
    "In this part of the code, I am going to show some analysis examples that can be carried out with this data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up the data processing in this particular example, I created a new pandas dataframe based on df_travel.\n",
    "df_travel_pandas = df_travel.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Which type of airport have used the immigrants considered in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel_airport = df_travel_pandas.merge(df_airport, left_on='city_state', right_on='city_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "small_airport     8506\n",
       "heliport          5101\n",
       "closed             762\n",
       "seaplane_base      421\n",
       "balloonport         12\n",
       "medium_airport       1\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "closed           1137459\n",
       "heliport         9875553\n",
       "seaplane_base     616327\n",
       "small_airport    1920789\n",
       "Name: travel_id, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_travel_airport.groupby('type').count()['travel_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: TOP10 most traveled places and which is the airline that travels the most to those places (in the data considered in the data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel_cities = df_travel_pandas.merge(df_cities, left_on='city_state', right_on='city_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline  city_state       \n",
       "AA       Miami_Florida        71380\n",
       "DL       New York_New York    33952\n",
       "VS       Orlando_Florida      33664\n",
       "BA       New York_New York    32311\n",
       "AA       New York_New York    31687\n",
       "AF       New York_New York    21462\n",
       "JJ       Miami_Florida        18069\n",
       "LA       Miami_Florida        16281\n",
       "AV       Miami_Florida        15857\n",
       "UA       Houston_Texas        15548\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_travel_cities[['airline','city_state']].value_counts()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
