{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is to create a data lake with data about immigration in EEUU, which facilitates the analysis and predictions to several types of company.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import pyreadstat\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_data = \"s3a://bucket-test-udacity/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in c:\\python39\\lib\\site-packages (2023.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.1.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in c:\\python39\\lib\\site-packages (from s3fs) (2023.1.0)\n",
      "Collecting aiobotocore~=2.4.2\n",
      "  Downloading aiobotocore-2.4.2-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 66.8/66.8 kB ? eta 0:00:00\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.     ------------------------------------- 323.6/323.6 kB 10.1 MB/s eta 0:00:00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrapt>=1.10.10 in c:\\python39\\lib\\site-packages (from aiobotocore~=2.4.2->s3fs) (1.14.1)\n",
      "Collecting botocore<1.27.60,>=1.27.59\n",
      "  Downloading botocore-1.27.59-py3-none-any.whl (9.1 MB)\n",
      "     ---------------------------------------- 9.1/9.1 MB 20.0 MB/s eta 0:00:00\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.10)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (22.2.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\python39\\lib\\site-packages (from aioitertools>=0.5.1->aiobotocore~=2.4.2->s3fs) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\python39\\lib\\site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\python39\\lib\\site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (2.8.1)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\python39\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python39\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.15.0)\n",
      "Installing collected packages: multidict, jmespath, frozenlist, async-timeout, aioitertools, yarl, botocore, aiosignal, aiohttp, aiobotocore, s3fs\n",
      "Successfully installed aiobotocore-2.4.2 aiohttp-3.8.4 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.2 botocore-1.27.59 frozenlist-1.3.3 jmespath-1.0.1 multidict-6.0.4 s3fs-2023.1.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()\n",
    "    return spark\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2735e722610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### SCOPE \n",
    "\n",
    "The purpose of this project is to create a data lake with data about immigration in the US and the circumstances in which it has occurred.\n",
    "\n",
    "To carry it out, we are considering three datasets, which are going to be raw data for our data lake.\n",
    "\n",
    "**The datasets used are:**\n",
    "- **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace.  **[This](https://www.trade.gov/i-94-arrivals-program)** is where the data comes from. The National Travel and Tourism Office (NTTO) manages the ADIS/I-94 visitor arrivals program in cooperation with the Department of Homeland Security (DHS)/U.S. Customs and Border Protection (CBP). The I-94 provides a count of visitor arrivals to the United States (with stays of 1-night or more and visiting under certain visa types) to calculate U.S. travel and tourism volume exports.\n",
    "- **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it **[here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)**.\n",
    "- **Airport Code Table:** This is a simple table of airport codes and corresponding cities. It comes from **[here](https://datahub.io/core/airport-codes#data)**.\n",
    "- **World Temperature Data**: \n",
    "\n",
    "The main tools which are going to be used are:\n",
    "- **Python libraries** like pandas or numpy\n",
    "- **Pyspark** to deal with the immigration dataset\n",
    "- **Aparhe Airflow** to automate a pipeline to extract this information programatically and to mantain the database updated.\n",
    "- **Amazon S3**: to store both the raw data and the final data lake in parquet.\n",
    "- **Amazon EMR**: to process the data with PySpark.\n",
    "\n",
    "#### DATA DESCRIPTION\n",
    "\n",
    "- **I94 Immigration Data:** This is a dataset with information from the people which arrive to EEUU as immigrants. \n",
    "\n",
    "- **U.S. City Demographic Data:** The information included in this dataset is the following:\n",
    "    - **City names**\n",
    "    - **State**: \n",
    "    - **Median age**\n",
    "    - **Male population**\n",
    "    - **Female population**\n",
    "    - **Total population**\n",
    "    - **Number of veterans**\n",
    "    - **Foreign born**\n",
    "    - **Average household size**\n",
    "    - **State code**\n",
    "    - **Race**\n",
    "    - **Statistic values**\n",
    "    \n",
    "    \n",
    "- **Airport Code Table:** Dataset with information about different airports. This information includes:\n",
    "    - **ident**: Identification code\n",
    "    - **type**: type of airport\n",
    "    - **name**: name of the airport\n",
    "    - **elevation_ft**: elevation above the sea level\n",
    "    - **iso_country**: iso code of each country\n",
    "    - **iso_region**: iso code of each region\n",
    "    - **municipality**: municipality where the airport is located\n",
    "    - **gps_code**: gps code of the airport\n",
    "    - **iata_code**: An IATA airport code, also known as an IATA location identifier, IATA station code, or simply a location identifier, is a three-character alphanumeric geocode designating many airports and metropolitan areas around the world, defined by the International Air Transport Association (IATA).\n",
    "    - **local_code**: local code of the airport\n",
    "    - **coordinates**: coordinates of the airport\n",
    "    \n",
    "    \n",
    "- **Global land temperatures by city**: Dataset with informmation about the temperature in different cities at different dates.\n",
    "    - **dt**: date of the data\n",
    "    - **AverageTemperature**\n",
    "    - **AverageTemperatureUncertainty**\n",
    "    - **City**\n",
    "    - **Country**\n",
    "    - **Latitude**\n",
    "    - **Longitude**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I94 Immigration Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sas = spark.read.parquet('sas_data/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_immigration_data(url = 'immigration_data_sample.csv'):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "    df_sas = spark.read.csv('immigration_data_sample.csv', header = True)\n",
    "#     df_sas = spark.read.parquet('sas_data/*.parquet'\n",
    "    return df_sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sas = read_immigration_data() #OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**U.S. City Demographic Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cities_data(url = 'us-cities-demographics.csv'):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "    df_cities = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "    return df_cities\n",
    "\n",
    "# df_cities = pd.read_csv('us-cities-demographics.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = read_cities_data() #OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Airport Code Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cities_data(url = 'airport-codes_csv.csv'):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "    df_airport = pd.read_csv(url)\n",
    "    return df_airport\n",
    "\n",
    "# df_airport = pd.read_csv('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = read_cities_data() #OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global land temperatures by city**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_temp_data(url = \"C:/Users/gonza/Downloads/GlobalLandTemperaturesByCity.csv\"):\n",
    "    '''\n",
    "    Function which loads the immigration dataset.\n",
    "    \n",
    "    INPUT:\n",
    "    url (string): URL of the bucket where the information is stored.\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): dataframe created based on the data\n",
    "    '''\n",
    "    df_temp = spark.read.csv(url, header = True, inferSchema = True)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = read_temp_data() #OK, pero el toPandas no funciona y no entiendo el por qué"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Although each file and dataset is different and have different problems to solve but the steps to be implemented are the following:\n",
    "\n",
    "* Modify the name of the columns to more descriptive names\n",
    "* Modify data types\n",
    "* Fox the missing values\n",
    "* Drop duplicates values\n",
    "* Replace codes with more descriptive names\n",
    "* Drop unnecesary columns\n",
    "\n",
    "Not all steps will have to be applied to all datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. I94 Immigration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>JL</td>\n",
       "      <td>56582674633.0</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>*GA</td>\n",
       "      <td>94361995930.0</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>55780468433.0</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>QR</td>\n",
       "      <td>94789696030.0</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>42322572633.0</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721257</td>\n",
       "      <td>1481650.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>10072016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>736852585.0</td>\n",
       "      <td>910</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _c0      cicid   i94yr i94mon i94cit i94res i94port  arrdate i94mode  \\\n",
       "0  2027561  4084316.0  2016.0    4.0  209.0  209.0     HHW  20566.0     1.0   \n",
       "1  2171295  4422636.0  2016.0    4.0  582.0  582.0     MCA  20567.0     1.0   \n",
       "2   589494  1195600.0  2016.0    4.0  148.0  112.0     OGG  20551.0     1.0   \n",
       "3  2631158  5291768.0  2016.0    4.0  297.0  297.0     LOS  20572.0     1.0   \n",
       "4  3032257   985523.0  2016.0    4.0  111.0  111.0     CHM  20550.0     3.0   \n",
       "5   721257  1481650.0  2016.0    4.0  577.0  577.0     ATL  20552.0     1.0   \n",
       "\n",
       "  i94addr  ... entdepu matflag biryear   dtaddto gender insnum airline  \\\n",
       "0      HI  ...    None       M  1955.0  07202016      F   None      JL   \n",
       "1      TX  ...    None       M  1990.0  10222016      M   None     *GA   \n",
       "2      FL  ...    None       M  1940.0  07052016      M   None      LH   \n",
       "3      CA  ...    None       M  1991.0  10272016      M   None      QR   \n",
       "4      NY  ...    None       M  1997.0  07042016      F   None    None   \n",
       "5      GA  ...    None       M  1965.0  10072016      M   None      DL   \n",
       "\n",
       "          admnum  fltno visatype  \n",
       "0  56582674633.0  00782       WT  \n",
       "1  94361995930.0  XBLNG       B2  \n",
       "2  55780468433.0  00464       WT  \n",
       "3  94789696030.0  00739       B2  \n",
       "4  42322572633.0   LAND       WT  \n",
       "5    736852585.0    910       B2  \n",
       "\n",
       "[6 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sas.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 29)\n"
     ]
    }
   ],
   "source": [
    "print((df_sas.count(), len(df_sas.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_c0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>954</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>859</td>\n",
       "      <td>35</td>\n",
       "      <td>967</td>\n",
       "      <td>1000</td>\n",
       "      <td>992</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>1542097.12</td>\n",
       "      <td>3040461.409</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>302.928</td>\n",
       "      <td>298.262</td>\n",
       "      <td>None</td>\n",
       "      <td>20559.68</td>\n",
       "      <td>1.078</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1973.618</td>\n",
       "      <td>8258277.404255319</td>\n",
       "      <td>None</td>\n",
       "      <td>3826.8571428571427</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.9372367950789E10</td>\n",
       "      <td>1337.2554291623578</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>915287.9043923795</td>\n",
       "      <td>1799817.7827726966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.48528516334758</td>\n",
       "      <td>202.12038988683958</td>\n",
       "      <td>None</td>\n",
       "      <td>8.995026987758733</td>\n",
       "      <td>0.4859548869516101</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17.90342449389525</td>\n",
       "      <td>1622586.3557888167</td>\n",
       "      <td>None</td>\n",
       "      <td>221.7425829858661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.338134181802248E10</td>\n",
       "      <td>6149.954574383991</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1006205</td>\n",
       "      <td>1000074.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>AGA</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>04082018</td>\n",
       "      <td>F</td>\n",
       "      <td>3468</td>\n",
       "      <td>*GA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00001</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25%</td>\n",
       "      <td>721257.0</td>\n",
       "      <td>1408683.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>7092016.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3668.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5991897633E10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50%</td>\n",
       "      <td>1494106.0</td>\n",
       "      <td>2938927.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20560.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>7252016.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3887.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.9313924133E10</td>\n",
       "      <td>410.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75%</td>\n",
       "      <td>2360660.0</td>\n",
       "      <td>4693164.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1.0122016E7</td>\n",
       "      <td>None</td>\n",
       "      <td>3943.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.343458123E10</td>\n",
       "      <td>906.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max</td>\n",
       "      <td>997880</td>\n",
       "      <td>999282.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>746.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>X96</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>X</td>\n",
       "      <td>4686</td>\n",
       "      <td>ZX</td>\n",
       "      <td>95021509030.0</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                _c0               cicid   i94yr i94mon  \\\n",
       "0   count               1000                1000    1000   1000   \n",
       "1    mean         1542097.12         3040461.409  2016.0    4.0   \n",
       "2  stddev  915287.9043923795  1799817.7827726966     0.0    0.0   \n",
       "3     min            1006205           1000074.0  2016.0    4.0   \n",
       "4     25%           721257.0           1408683.0  2016.0    4.0   \n",
       "5     50%          1494106.0           2938927.0  2016.0    4.0   \n",
       "6     75%          2360660.0           4693164.0  2016.0    4.0   \n",
       "7     max             997880            999282.0  2016.0    4.0   \n",
       "\n",
       "               i94cit              i94res i94port            arrdate  \\\n",
       "0                1000                1000    1000               1000   \n",
       "1             302.928             298.262    None           20559.68   \n",
       "2  206.48528516334758  202.12038988683958    None  8.995026987758733   \n",
       "3               103.0               103.0     AGA            20545.0   \n",
       "4               135.0               131.0    None            20552.0   \n",
       "5               213.0               213.0    None            20560.0   \n",
       "6               438.0               438.0    None            20567.0   \n",
       "7               746.0               696.0     X96            20574.0   \n",
       "\n",
       "              i94mode  ... entdepu matflag            biryear  \\\n",
       "0                1000  ...       0     954               1000   \n",
       "1               1.078  ...    None    None           1973.618   \n",
       "2  0.4859548869516101  ...    None    None  17.90342449389525   \n",
       "3                 1.0  ...    None       M             1923.0   \n",
       "4                 1.0  ...    None    None             1961.0   \n",
       "5                 1.0  ...    None    None             1974.0   \n",
       "6                 1.0  ...    None    None             1985.0   \n",
       "7                 9.0  ...    None       M             2015.0   \n",
       "\n",
       "              dtaddto gender              insnum airline  \\\n",
       "0                1000    859                  35     967   \n",
       "1   8258277.404255319   None  3826.8571428571427     2.0   \n",
       "2  1622586.3557888167   None   221.7425829858661     0.0   \n",
       "3            04082018      F                3468     *GA   \n",
       "4           7092016.0   None              3668.0     2.0   \n",
       "5           7252016.0   None              3887.0     2.0   \n",
       "6         1.0122016E7   None              3943.0     2.0   \n",
       "7                 D/S      X                4686      ZX   \n",
       "\n",
       "                 admnum               fltno visatype  \n",
       "0                  1000                 992     1000  \n",
       "1    6.9372367950789E10  1337.2554291623578     None  \n",
       "2  2.338134181802248E10   6149.954574383991     None  \n",
       "3                   0.0               00001       B1  \n",
       "4       5.5991897633E10               100.0     None  \n",
       "5       5.9313924133E10               410.0     None  \n",
       "6        9.343458123E10               906.0     None  \n",
       "7         95021509030.0               XBLNG       WT  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sas.summary().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, cicid=0, i94yr=0, i94mon=0, i94cit=0, i94res=0, i94port=0, arrdate=0, i94mode=0, i94addr=59, depdate=49, i94bir=0, i94visa=0, count=0, dtadfile=0, visapost=618, occup=996, entdepa=0, entdepd=46, entdepu=1000, matflag=46, biryear=0, dtaddto=0, gender=141, insnum=965, airline=33, admnum=0, fltno=8, visatype=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values analysis\n",
    "df_sas.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_sas.columns]).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in the variables `i94addr=59`, `depdate=49`, `visapost=618`, `occup=996`, `entdepd=46`, `entdepu=1000`, `matflag=46`, `gender=141`, `insnum=965`, `airline=33` and `fltno=8`. It is going to be necessary to analyze each of them to identify a strategy to avoid having missing values, if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_immigration_data(df_sas):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Change column names\n",
    "    2. Change the data types\n",
    "    3. Fix the missing values\n",
    "    4. Drop duplicate values\n",
    "    5. Replace codes with more descriptive values\n",
    "    6. Drop unnecesary columns\n",
    "    \n",
    "    INPUT:\n",
    "    df_sas (Spark DataFrame): DataFrame with\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_sas (Spark DataFrame): data already cleaned for being uploaded to the data lake\n",
    "    '''\n",
    "    \n",
    "    # 1. Change column names\n",
    "    # Keys to modify the codes for names easier to understand\n",
    "    names = {'cicid':'immigrant_id','I94YR': 'year', 'I94MON':'month', 'I94CIT':'country_1','I94RES':'country_2','I94PORT':'city',\n",
    "        'ARRDATE':'arrival_date','I94MODE':'transport_mode','I94ADDR':'state','DEPDATE':'departure_date',\n",
    "         'I94BIR':'age_respondent','I94VISA':'visa_code','COUNT':'summary_statistics','DTADFILE':'character_date_field',\n",
    "        'VISAPOST':'department_visa','OCCUP':'occupation','ENTDEPA':'arrival_flag','ENTDEPD':'departure_flag',\n",
    "        'ENTDEPU':'update_flag','MATFLAG':'match_flag','BIRYEAR':'birth_year','DTADDTO':'character_date_field',\n",
    "        'GENDER':'non_inmigrant_sex','INSNUM':'ins_number','AIRLINE':'airline','ADMNUM':'admission_number',\n",
    "        'FLTNO':'flight_number','VISATYPE':'visa_type'}\n",
    "    \n",
    "    # Modification of the column names for others more intuitive\n",
    "    for i in names:\n",
    "        df_sas = df_sas.withColumnRenamed(i,names[i])\n",
    "\n",
    "    # 2. Change the data types\n",
    "    from pyspark.sql.functions import substring, length, col, expr\n",
    "    df_sas = df_sas.withColumn(\"country_1\",expr(\"substring(country_1, 1, length(country_1)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"immigrant_id\",expr(\"substring(immigrant_id, 1, length(immigrant_id)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"year\",expr(\"substring(year, 1, length(year)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"month\",expr(\"substring(month, 1, length(month)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"country_2\",expr(\"substring(country_2, 1, length(country_2)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"arrival_date\",expr(\"substring(arrival_date, 1, length(arrival_date)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"transport_mode\",expr(\"substring(transport_mode, 1, length(transport_mode)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"departure_date\",expr(\"substring(departure_date, 1, length(departure_date)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"age_respondent\",expr(\"substring(age_respondent, 1, length(age_respondent)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"visa_code\",expr(\"substring(visa_code, 1, length(visa_code)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"summary_statistics\",expr(\"substring(summary_statistics, 1, length(summary_statistics)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"birth_year\",expr(\"substring(birth_year, 1, length(birth_year)-2)\"))\n",
    "    df_sas = df_sas.withColumn(\"admission_number\",expr(\"substring(admission_number, 1, length(admission_number)-2)\"))\n",
    "    \n",
    "    df_sas = df_sas.withColumn(\"country_1\", col(\"country_1\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"immigrant_id\", col(\"immigrant_id\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"year\", col(\"year\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"month\", col(\"month\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"country_2\", col(\"country_2\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"arrival_date\", col(\"arrival_date\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"transport_mode\", col(\"transport_mode\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"departure_date\", col(\"departure_date\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"age_respondent\", col(\"age_respondent\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"visa_code\", col(\"visa_code\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"summary_statistics\", col(\"summary_statistics\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"birth_year\", col(\"birth_year\").cast('int'))\n",
    "    df_sas = df_sas.withColumn(\"admission_number\", col(\"admission_number\").cast('int'))\n",
    "    \n",
    "    # 3. Fix the missing values\n",
    "    # 3.1. I94ADDR (state)\n",
    "    df_sas = df_sas.filter(df_sas.state.isNotNull())\n",
    "    \n",
    "    # 3.2. DEPDATE (departure_date)\n",
    "    df_sas.filter(df_sas.departure_date.isNotNull()).toPandas()\n",
    "    \n",
    "    # 3.3. VISAPOST (department_visa)\n",
    "    df_sas = df_sas.drop('department_visa')\n",
    "    \n",
    "    # 3.4. OCCUP (occupation)\n",
    "    df_sas = df_sas.drop('occupation')\n",
    "    \n",
    "    # 3.5. ENTDEPD (departure_flag)\n",
    "    df_sas = df_sas.filter(df_sas.departure_flag.isNotNull())\n",
    "    \n",
    "    # 3.6. MATFLAG (match_flag)\n",
    "    # There are no missing values anymore, because those ones has been already solved in a prior step.    \n",
    "    \n",
    "    # 4. Drop duplicate values\n",
    "    df_sas = df_sas.dropDuplicates()    \n",
    "    \n",
    "    # 5. Replace codes with more descriptive values\n",
    "    # Read the SAS file with the meaning of the codes of the columns I94CIT & I94RES,I94PORT, I94MODE, I94ADDR\n",
    "    country_codes = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    with open(country_codes) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.replace('\"','').replace('\\n','').replace(\"'\",'') for line in lines]\n",
    "    \n",
    "    df_sas_codes = pd.DataFrame(lines)\n",
    "    \n",
    "    # 5.1. I94CIT & I94RES\n",
    "    values_I94CIT_I94RES = df_sas_codes[9:298]\n",
    "    values_I94CIT_I94RES = values_I94CIT_I94RES[0].str.split('=', expand = True)\n",
    "    values_I94CIT_I94RES.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94CIT_I94RES = spark.createDataFrame(values_I94CIT_I94RES)\n",
    "    df_sas = df_sas.join(values_I94CIT_I94RES, df_sas.country_1 == values_I94CIT_I94RES.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','country_1_name')\n",
    "    df_sas = df_sas.join(values_I94CIT_I94RES, df_sas.country_2 == values_I94CIT_I94RES.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','country_2_name')\n",
    "    \n",
    "    # 5.2. I94PORT\n",
    "    values_I94PORT = df_sas_codes[302:962]\n",
    "    values_I94PORT = values_I94PORT[0].str.split('=', expand = True)\n",
    "    values_I94PORT.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94PORT['code'] = values_I94PORT['code'].str.strip()\n",
    "    values_I94PORT['name'] = values_I94PORT['name'].str.strip()\n",
    "    values_I94PORT.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94PORT = spark.createDataFrame(values_I94PORT)\n",
    "    df_sas = df_sas.join(values_I94PORT, df_sas.city == values_I94PORT.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','city_name')    \n",
    "    \n",
    "    # 5.3. I94MODE\n",
    "    values_I94MODE = df_sas_codes[972:976]\n",
    "    values_I94MODE = values_I94MODE[0].str.split('=', expand = True)\n",
    "    values_I94MODE.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94MODE['code'] = values_I94MODE['code'].str.strip()\n",
    "    values_I94MODE['name'] = values_I94MODE['name'].str.strip()\n",
    "    values_I94MODE = spark.createDataFrame(values_I94MODE)\n",
    "    df_sas = df_sas.join(values_I94MODE, df_sas.transport_mode == values_I94MODE.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name','transport_mode_name')\n",
    "    \n",
    "    # 5.4. I94ADDR\n",
    "    values_I94ADDR= df_sas_codes[982:1036]\n",
    "    values_I94ADDR = values_I94ADDR[0].str.split('=', expand = True)\n",
    "    values_I94ADDR.rename(columns = {0:\"code\" , 1:\"name\"}, inplace = True)\n",
    "    values_I94ADDR['code'] = values_I94ADDR['code'].str.strip()\n",
    "    values_I94ADDR['name'] = values_I94ADDR['name'].str.strip()\n",
    "    values_I94ADDR = spark.createDataFrame(values_I94ADDR)\n",
    "    df_sas = df_sas.join(values_I94ADDR, df_sas.state == values_I94ADDR.code)\n",
    "    df_sas = df_sas.drop('code').withColumnRenamed('name', 'state_name')\n",
    "    \n",
    "    \n",
    "    # 6. Drop unnecesary columns\n",
    "    df_sas = df_sas.drop('_c0')\n",
    "    \n",
    "    return df_sas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sas = read_immigration_data() #OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sas = cleaning_immigration_data(df_sas) #No funciona, devuelve un dataframe vacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>immigrant_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>country_1</th>\n",
       "      <th>country_2</th>\n",
       "      <th>city</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>transport_mode</th>\n",
       "      <th>state</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>...</th>\n",
       "      <th>ins_number</th>\n",
       "      <th>airline</th>\n",
       "      <th>admission_number</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>visa_type</th>\n",
       "      <th>country_1_name</th>\n",
       "      <th>country_2_name</th>\n",
       "      <th>city_name</th>\n",
       "      <th>transport_mode_name</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5365953</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20572</td>\n",
       "      <td>1</td>\n",
       "      <td>PA</td>\n",
       "      <td>20581</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>UA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04429</td>\n",
       "      <td>B2</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>HOUSTON, TX</td>\n",
       "      <td>Air</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4800385</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20569</td>\n",
       "      <td>1</td>\n",
       "      <td>MI</td>\n",
       "      <td>20611</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00331</td>\n",
       "      <td>B1</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>ATLANTA, GA</td>\n",
       "      <td>Air</td>\n",
       "      <td>MICHIGAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5358837</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20572</td>\n",
       "      <td>1</td>\n",
       "      <td>TX</td>\n",
       "      <td>20575</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00418</td>\n",
       "      <td>B2</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>NEW YORK, NY</td>\n",
       "      <td>Air</td>\n",
       "      <td>TEXAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>930868</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20549</td>\n",
       "      <td>1</td>\n",
       "      <td>DC</td>\n",
       "      <td>20552</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>UA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01567</td>\n",
       "      <td>B2</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>WASHINGTON DC</td>\n",
       "      <td>Air</td>\n",
       "      <td>DIST. OF COLUMBIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2825582</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>MIA</td>\n",
       "      <td>20559</td>\n",
       "      <td>1</td>\n",
       "      <td>FL</td>\n",
       "      <td>20565</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00420</td>\n",
       "      <td>B2</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "      <td>MIAMI, FL</td>\n",
       "      <td>Air</td>\n",
       "      <td>FLORIDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   immigrant_id  year  month  country_1  country_2 city  arrival_date  \\\n",
       "0       5365953  2016      4        582        582  HOU         20572   \n",
       "1       4800385  2016      4        582        582  ATL         20569   \n",
       "2       5358837  2016      4        582        582  NYC         20572   \n",
       "3        930868  2016      4        582        582  WAS         20549   \n",
       "4       2825582  2016      4        582        582  MIA         20559   \n",
       "\n",
       "   transport_mode state  departure_date  ...  ins_number  airline  \\\n",
       "0               1    PA           20581  ...        None       UA   \n",
       "1               1    MI           20611  ...        None       DL   \n",
       "2               1    TX           20575  ...        None       AM   \n",
       "3               1    DC           20552  ...        None       UA   \n",
       "4               1    FL           20565  ...        None       AM   \n",
       "\n",
       "   admission_number flight_number visa_type  \\\n",
       "0               NaN         04429        B2   \n",
       "1               NaN         00331        B1   \n",
       "2               NaN         00418        B2   \n",
       "3               NaN         01567        B2   \n",
       "4               NaN         00420        B2   \n",
       "\n",
       "                                      country_1_name  \\\n",
       "0    MEXICO Air Sea, and Not Reported (I-94, no l...   \n",
       "1    MEXICO Air Sea, and Not Reported (I-94, no l...   \n",
       "2    MEXICO Air Sea, and Not Reported (I-94, no l...   \n",
       "3    MEXICO Air Sea, and Not Reported (I-94, no l...   \n",
       "4    MEXICO Air Sea, and Not Reported (I-94, no l...   \n",
       "\n",
       "                                      country_2_name      city_name  \\\n",
       "0    MEXICO Air Sea, and Not Reported (I-94, no l...    HOUSTON, TX   \n",
       "1    MEXICO Air Sea, and Not Reported (I-94, no l...    ATLANTA, GA   \n",
       "2    MEXICO Air Sea, and Not Reported (I-94, no l...   NEW YORK, NY   \n",
       "3    MEXICO Air Sea, and Not Reported (I-94, no l...  WASHINGTON DC   \n",
       "4    MEXICO Air Sea, and Not Reported (I-94, no l...      MIAMI, FL   \n",
       "\n",
       "   transport_mode_name         state_name  \n",
       "0                  Air       PENNSYLVANIA  \n",
       "1                  Air           MICHIGAN  \n",
       "2                  Air              TEXAS  \n",
       "3                  Air  DIST. OF COLUMBIA  \n",
       "4                  Air            FLORIDA  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sas.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sas.write.partitionBy('year').mode('overwrite').parquet(output_data + immigration/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Airport Code Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the code structure of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport = pd.read_csv('airport-codes_csv.csv')\n",
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ident         55075 non-null  object \n",
      " 1   type          55075 non-null  object \n",
      " 2   name          55075 non-null  object \n",
      " 3   elevation_ft  48069 non-null  float64\n",
      " 4   continent     27356 non-null  object \n",
      " 5   iso_country   54828 non-null  object \n",
      " 6   iso_region    55075 non-null  object \n",
      " 7   municipality  49399 non-null  object \n",
      " 8   gps_code      41030 non-null  object \n",
      " 9   iata_code     9189 non-null   object \n",
      " 10  local_code    28686 non-null  object \n",
      " 11  coordinates   55075 non-null  object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_airport.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>48069.000000</td>\n",
       "      <td>27356</td>\n",
       "      <td>54828</td>\n",
       "      <td>55075</td>\n",
       "      <td>49399</td>\n",
       "      <td>41030</td>\n",
       "      <td>9189</td>\n",
       "      <td>28686</td>\n",
       "      <td>55075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>55075</td>\n",
       "      <td>7</td>\n",
       "      <td>52144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>243</td>\n",
       "      <td>2810</td>\n",
       "      <td>27133</td>\n",
       "      <td>40850</td>\n",
       "      <td>9042</td>\n",
       "      <td>27436</td>\n",
       "      <td>54874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>W34</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Centre Hospitalier Heliport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EU</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>MBAC</td>\n",
       "      <td>0</td>\n",
       "      <td>LAN</td>\n",
       "      <td>0, 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>33965</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7840</td>\n",
       "      <td>22757</td>\n",
       "      <td>2277</td>\n",
       "      <td>404</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1240.789677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1602.363459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1266.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ident           type                         name  elevation_ft  \\\n",
       "count   55075          55075                        55075  48069.000000   \n",
       "unique  55075              7                        52144           NaN   \n",
       "top       W34  small_airport  Centre Hospitalier Heliport           NaN   \n",
       "freq        1          33965                           85           NaN   \n",
       "mean      NaN            NaN                          NaN   1240.789677   \n",
       "std       NaN            NaN                          NaN   1602.363459   \n",
       "min       NaN            NaN                          NaN  -1266.000000   \n",
       "25%       NaN            NaN                          NaN    205.000000   \n",
       "50%       NaN            NaN                          NaN    718.000000   \n",
       "75%       NaN            NaN                          NaN   1497.000000   \n",
       "max       NaN            NaN                          NaN  22000.000000   \n",
       "\n",
       "       continent iso_country iso_region municipality gps_code iata_code  \\\n",
       "count      27356       54828      55075        49399    41030      9189   \n",
       "unique         6         243       2810        27133    40850      9042   \n",
       "top           EU          US      US-TX        Seoul     MBAC         0   \n",
       "freq        7840       22757       2277          404        3        80   \n",
       "mean         NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "std          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "min          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "25%          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "50%          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "75%          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "max          NaN         NaN        NaN          NaN      NaN       NaN   \n",
       "\n",
       "       local_code coordinates  \n",
       "count       28686       55075  \n",
       "unique      27436       54874  \n",
       "top           LAN        0, 0  \n",
       "freq            5          53  \n",
       "mean          NaN         NaN  \n",
       "std           NaN         NaN  \n",
       "min           NaN         NaN  \n",
       "25%           NaN         NaN  \n",
       "50%           NaN         NaN  \n",
       "75%           NaN         NaN  \n",
       "max           NaN         NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variables with a high amount of missing values, so it is going to be necessary to analyze, if they have to be removed or those values can be filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent       27719\n",
       "iso_country       247\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident            0.000000\n",
       "type             0.000000\n",
       "name             0.000000\n",
       "elevation_ft    12.720835\n",
       "continent       50.329551\n",
       "iso_country      0.448479\n",
       "iso_region       0.000000\n",
       "municipality    10.305946\n",
       "gps_code        25.501589\n",
       "iata_code       83.315479\n",
       "local_code      47.914662\n",
       "coordinates      0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.isna().sum()/df_airport.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55065</th>\n",
       "      <td>ZYTH</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Tahe Airport</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Tahe</td>\n",
       "      <td>ZYTH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.720222222, 52.2244444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55071</th>\n",
       "      <td>ZYYY</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Shenyang Dongta Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Shenyang</td>\n",
       "      <td>ZYYY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.49600219726562, 41.784400939941406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55072</th>\n",
       "      <td>ZZ-0001</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Sealand Helipad</td>\n",
       "      <td>40.0</td>\n",
       "      <td>EU</td>\n",
       "      <td>GB</td>\n",
       "      <td>GB-ENG</td>\n",
       "      <td>Sealand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4825, 51.894444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55073</th>\n",
       "      <td>ZZ-0002</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Glorioso Islands Airstrip</td>\n",
       "      <td>11.0</td>\n",
       "      <td>AF</td>\n",
       "      <td>TF</td>\n",
       "      <td>TF-U-A</td>\n",
       "      <td>Grande Glorieuse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.296388888900005, -11.584277777799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55074</th>\n",
       "      <td>ZZZZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Satsuma IÅjima Airport</td>\n",
       "      <td>338.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>JP</td>\n",
       "      <td>JP-46</td>\n",
       "      <td>Mishima-Mura</td>\n",
       "      <td>RJX7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.270556, 30.784722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45886 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ident            type                                name  \\\n",
       "0          00A        heliport                   Total Rf Heliport   \n",
       "1         00AA   small_airport                Aero B Ranch Airport   \n",
       "2         00AK   small_airport                        Lowell Field   \n",
       "3         00AL   small_airport                        Epps Airpark   \n",
       "4         00AR          closed  Newport Hospital & Clinic Heliport   \n",
       "...        ...             ...                                 ...   \n",
       "55065     ZYTH   small_airport                        Tahe Airport   \n",
       "55071     ZYYY  medium_airport             Shenyang Dongta Airport   \n",
       "55072  ZZ-0001        heliport                     Sealand Helipad   \n",
       "55073  ZZ-0002   small_airport           Glorioso Islands Airstrip   \n",
       "55074     ZZZZ   small_airport             Satsuma IÅjima Airport   \n",
       "\n",
       "       elevation_ft continent iso_country iso_region      municipality  \\\n",
       "0              11.0       NaN          US      US-PA          Bensalem   \n",
       "1            3435.0       NaN          US      US-KS             Leoti   \n",
       "2             450.0       NaN          US      US-AK      Anchor Point   \n",
       "3             820.0       NaN          US      US-AL           Harvest   \n",
       "4             237.0       NaN          US      US-AR           Newport   \n",
       "...             ...       ...         ...        ...               ...   \n",
       "55065        1240.0        AS          CN      CN-23              Tahe   \n",
       "55071           NaN        AS          CN      CN-21          Shenyang   \n",
       "55072          40.0        EU          GB     GB-ENG           Sealand   \n",
       "55073          11.0        AF          TF     TF-U-A  Grande Glorieuse   \n",
       "55074         338.0        AS          JP      JP-46      Mishima-Mura   \n",
       "\n",
       "      gps_code iata_code local_code                              coordinates  \n",
       "0          00A       NaN        00A       -74.93360137939453, 40.07080078125  \n",
       "1         00AA       NaN       00AA                   -101.473911, 38.704022  \n",
       "2         00AK       NaN       00AK              -151.695999146, 59.94919968  \n",
       "3         00AL       NaN       00AL    -86.77030181884766, 34.86479949951172  \n",
       "4          NaN       NaN        NaN                      -91.254898, 35.6087  \n",
       "...        ...       ...        ...                                      ...  \n",
       "55065     ZYTH       NaN        NaN             124.720222222, 52.2244444444  \n",
       "55071     ZYYY       NaN        NaN   123.49600219726562, 41.784400939941406  \n",
       "55072      NaN       NaN        NaN                        1.4825, 51.894444  \n",
       "55073      NaN       NaN        NaN  47.296388888900005, -11.584277777799999  \n",
       "55074     RJX7       NaN        NaN                    130.270556, 30.784722  \n",
       "\n",
       "[45886 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport[df_airport.iata_code.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ident, type, name, elevation_ft, continent, iso_country, iso_region, municipality, gps_code, iata_code, local_code, coordinates]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport[df_airport.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_airport_data(df_airport):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Modify the names of the columns to more descriptive values\n",
    "    2. Drop unnecessary columns\n",
    "    3. Modify data types\n",
    "    4. Replace codes with more descriptive values\n",
    "    5. Drop duplicated values\n",
    "    \n",
    "    INPUT:\n",
    "    df_airport (Spark DataFrame): DataFrame with\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_airport (Spark DataFrame): data already cleaned for being uploaded to the data lake\n",
    "    '''\n",
    "    \n",
    "    # 1. Modify the names of the columns to more descriptive values\n",
    "    airport_coordinates = df_airport['coordinates'].str.split(',', expand = True).rename(columns={0:'latitude', 1:'longitude'})\n",
    "    df_airport['latitude'] = airport_coordinates['latitude']\n",
    "    df_airport['longitude'] = airport_coordinates['longitude']\n",
    "    df_airport.drop('coordinates', axis = 1, inplace = True)\n",
    "    \n",
    "    # 2. Drop unnecessary columns\n",
    "    df_airport.drop(['iata_code','continent', 'gps_code', 'local_code', 'elevation_ft', 'municipality',\n",
    "                 'iso_country'], axis = 1, inplace = True)\n",
    "    \n",
    "    # 3. Modify data types\n",
    "    df_airport.latitude = df_airport.latitude.astype('float').round(2)\n",
    "    df_airport.longitude = df_airport.longitude.astype('float').round(2)\n",
    "    df_iso_states = pd.read_csv('ISO_code_US.csv', sep=\";\")\n",
    "\n",
    "    # 4. Replace codes with more descriptive values\n",
    "    df_iso_states = pd.read_csv('ISO_code_US.csv', sep=\";\")\n",
    "    df_airport = pd.merge(df_airport, df_iso_states, how = 'inner', left_on=['iso_region'], right_on = ['iso_code'])\n",
    "    df_airport.drop(\"iso_code\", axis = 1, inplace = True)\n",
    "    \n",
    "    # 5. Drop duplicated values\n",
    "    df_airport = df_airport.drop_duplicates()\n",
    "    \n",
    "    return df_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-74.93</td>\n",
       "      <td>40.07</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00PA</td>\n",
       "      <td>heliport</td>\n",
       "      <td>R J D Heliport</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-75.75</td>\n",
       "      <td>39.95</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00PN</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ferrell Field</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-80.21</td>\n",
       "      <td>41.30</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00PS</td>\n",
       "      <td>closed</td>\n",
       "      <td>Thomas Field</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-77.37</td>\n",
       "      <td>40.38</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01PA</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Pine Heliport</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>-80.05</td>\n",
       "      <td>40.66</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type               name iso_region  latitude  longitude  \\\n",
       "0   00A       heliport  Total Rf Heliport      US-PA    -74.93      40.07   \n",
       "1  00PA       heliport     R J D Heliport      US-PA    -75.75      39.95   \n",
       "2  00PN  small_airport      Ferrell Field      US-PA    -80.21      41.30   \n",
       "3  00PS         closed       Thomas Field      US-PA    -77.37      40.38   \n",
       "4  01PA       heliport      Pine Heliport      US-PA    -80.05      40.66   \n",
       "\n",
       "          state  \n",
       "0  Pennsylvania  \n",
       "1  Pennsylvania  \n",
       "2  Pennsylvania  \n",
       "3  Pennsylvania  \n",
       "4  Pennsylvania  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport = cleaning_airport_data(df_airport)\n",
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport.to_csv(output_data + airport/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. U.S. Cities Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# U.S. City Demographic Data: This data comes from OpenSoft.\n",
    "df_cities = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   City                    2891 non-null   object \n",
      " 1   State                   2891 non-null   object \n",
      " 2   Median Age              2891 non-null   float64\n",
      " 3   Male Population         2888 non-null   float64\n",
      " 4   Female Population       2888 non-null   float64\n",
      " 5   Total Population        2891 non-null   int64  \n",
      " 6   Number of Veterans      2878 non-null   float64\n",
      " 7   Foreign-born            2878 non-null   float64\n",
      " 8   Average Household Size  2875 non-null   float64\n",
      " 9   State Code              2891 non-null   object \n",
      " 10  Race                    2891 non-null   object \n",
      " 11  Count                   2891 non-null   int64  \n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cities.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2891</td>\n",
       "      <td>2891</td>\n",
       "      <td>2891.000000</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2878.000000</td>\n",
       "      <td>2.878000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2891</td>\n",
       "      <td>2891</td>\n",
       "      <td>2.891000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>567</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>California</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>676</td>\n",
       "      <td>596</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.494881</td>\n",
       "      <td>9.732843e+04</td>\n",
       "      <td>1.017696e+05</td>\n",
       "      <td>1.989668e+05</td>\n",
       "      <td>9367.832523</td>\n",
       "      <td>4.065360e+04</td>\n",
       "      <td>2.742543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.896377e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.401617</td>\n",
       "      <td>2.162999e+05</td>\n",
       "      <td>2.315646e+05</td>\n",
       "      <td>4.475559e+05</td>\n",
       "      <td>13211.219924</td>\n",
       "      <td>1.557491e+05</td>\n",
       "      <td>0.433291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.443856e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>2.928100e+04</td>\n",
       "      <td>2.734800e+04</td>\n",
       "      <td>6.321500e+04</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>8.610000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.800000</td>\n",
       "      <td>3.928900e+04</td>\n",
       "      <td>4.122700e+04</td>\n",
       "      <td>8.042900e+04</td>\n",
       "      <td>3739.000000</td>\n",
       "      <td>9.224000e+03</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.435000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>5.234100e+04</td>\n",
       "      <td>5.380900e+04</td>\n",
       "      <td>1.067820e+05</td>\n",
       "      <td>5397.000000</td>\n",
       "      <td>1.882200e+04</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.378000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.664175e+04</td>\n",
       "      <td>8.960400e+04</td>\n",
       "      <td>1.752320e+05</td>\n",
       "      <td>9368.000000</td>\n",
       "      <td>3.397175e+04</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.444700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>4.081698e+06</td>\n",
       "      <td>4.468707e+06</td>\n",
       "      <td>8.550405e+06</td>\n",
       "      <td>156961.000000</td>\n",
       "      <td>3.212500e+06</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.835726e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City       State   Median Age  Male Population  Female Population  \\\n",
       "count       2891        2891  2891.000000     2.888000e+03       2.888000e+03   \n",
       "unique       567          49          NaN              NaN                NaN   \n",
       "top     Columbia  California          NaN              NaN                NaN   \n",
       "freq          15         676          NaN              NaN                NaN   \n",
       "mean         NaN         NaN    35.494881     9.732843e+04       1.017696e+05   \n",
       "std          NaN         NaN     4.401617     2.162999e+05       2.315646e+05   \n",
       "min          NaN         NaN    22.900000     2.928100e+04       2.734800e+04   \n",
       "25%          NaN         NaN    32.800000     3.928900e+04       4.122700e+04   \n",
       "50%          NaN         NaN    35.300000     5.234100e+04       5.380900e+04   \n",
       "75%          NaN         NaN    38.000000     8.664175e+04       8.960400e+04   \n",
       "max          NaN         NaN    70.500000     4.081698e+06       4.468707e+06   \n",
       "\n",
       "        Total Population  Number of Veterans  Foreign-born  \\\n",
       "count       2.891000e+03         2878.000000  2.878000e+03   \n",
       "unique               NaN                 NaN           NaN   \n",
       "top                  NaN                 NaN           NaN   \n",
       "freq                 NaN                 NaN           NaN   \n",
       "mean        1.989668e+05         9367.832523  4.065360e+04   \n",
       "std         4.475559e+05        13211.219924  1.557491e+05   \n",
       "min         6.321500e+04          416.000000  8.610000e+02   \n",
       "25%         8.042900e+04         3739.000000  9.224000e+03   \n",
       "50%         1.067820e+05         5397.000000  1.882200e+04   \n",
       "75%         1.752320e+05         9368.000000  3.397175e+04   \n",
       "max         8.550405e+06       156961.000000  3.212500e+06   \n",
       "\n",
       "        Average Household Size State Code                Race         Count  \n",
       "count              2875.000000       2891                2891  2.891000e+03  \n",
       "unique                     NaN         49                   5           NaN  \n",
       "top                        NaN         CA  Hispanic or Latino           NaN  \n",
       "freq                       NaN        676                 596           NaN  \n",
       "mean                  2.742543        NaN                 NaN  4.896377e+04  \n",
       "std                   0.433291        NaN                 NaN  1.443856e+05  \n",
       "min                   2.000000        NaN                 NaN  9.800000e+01  \n",
       "25%                   2.430000        NaN                 NaN  3.435000e+03  \n",
       "50%                   2.650000        NaN                 NaN  1.378000e+04  \n",
       "75%                   2.950000        NaN                 NaN  5.444700e+04  \n",
       "max                   4.980000        NaN                 NaN  3.835726e+06  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>335559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>76349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>12143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>139967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>4031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Ponce</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.5</td>\n",
       "      <td>56968.0</td>\n",
       "      <td>64615.0</td>\n",
       "      <td>121583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>120705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>Bayamón</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>39.4</td>\n",
       "      <td>80128.0</td>\n",
       "      <td>90131.0</td>\n",
       "      <td>170259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>169155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>69936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>65521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          City        State  Median Age  Male Population  Female Population  \\\n",
       "111   San Juan  Puerto Rico        41.4         155408.0           186829.0   \n",
       "155     Caguas  Puerto Rico        40.4          34743.0            42265.0   \n",
       "258   Carolina  Puerto Rico        42.0          64758.0            77308.0   \n",
       "637   Carolina  Puerto Rico        42.0          64758.0            77308.0   \n",
       "1747  San Juan  Puerto Rico        41.4         155408.0           186829.0   \n",
       "1748  Mayagüez  Puerto Rico        38.1          30799.0            35782.0   \n",
       "1995     Ponce  Puerto Rico        40.5          56968.0            64615.0   \n",
       "2004   Bayamón  Puerto Rico        39.4          80128.0            90131.0   \n",
       "2441  San Juan  Puerto Rico        41.4         155408.0           186829.0   \n",
       "2589  Guaynabo  Puerto Rico        42.2          33066.0            37426.0   \n",
       "2597    Caguas  Puerto Rico        40.4          34743.0            42265.0   \n",
       "2654  Guaynabo  Puerto Rico        42.2          33066.0            37426.0   \n",
       "2746  Mayagüez  Puerto Rico        38.1          30799.0            35782.0   \n",
       "\n",
       "      Total Population  Number of Veterans  Foreign-born  \\\n",
       "111             342237                 NaN           NaN   \n",
       "155              77008                 NaN           NaN   \n",
       "258             142066                 NaN           NaN   \n",
       "637             142066                 NaN           NaN   \n",
       "1747            342237                 NaN           NaN   \n",
       "1748             66581                 NaN           NaN   \n",
       "1995            121583                 NaN           NaN   \n",
       "2004            170259                 NaN           NaN   \n",
       "2441            342237                 NaN           NaN   \n",
       "2589             70492                 NaN           NaN   \n",
       "2597             77008                 NaN           NaN   \n",
       "2654             70492                 NaN           NaN   \n",
       "2746             66581                 NaN           NaN   \n",
       "\n",
       "      Average Household Size State Code                               Race  \\\n",
       "111                      NaN         PR                 Hispanic or Latino   \n",
       "155                      NaN         PR                 Hispanic or Latino   \n",
       "258                      NaN         PR  American Indian and Alaska Native   \n",
       "637                      NaN         PR                 Hispanic or Latino   \n",
       "1747                     NaN         PR  American Indian and Alaska Native   \n",
       "1748                     NaN         PR                              Asian   \n",
       "1995                     NaN         PR                 Hispanic or Latino   \n",
       "2004                     NaN         PR                 Hispanic or Latino   \n",
       "2441                     NaN         PR                              Asian   \n",
       "2589                     NaN         PR                 Hispanic or Latino   \n",
       "2597                     NaN         PR  American Indian and Alaska Native   \n",
       "2654                     NaN         PR  American Indian and Alaska Native   \n",
       "2746                     NaN         PR                 Hispanic or Latino   \n",
       "\n",
       "       Count  \n",
       "111   335559  \n",
       "155    76349  \n",
       "258    12143  \n",
       "637   139967  \n",
       "1747    4031  \n",
       "1748     235  \n",
       "1995  120705  \n",
       "2004  169155  \n",
       "2441    2452  \n",
       "2589   69936  \n",
       "2597     624  \n",
       "2654     589  \n",
       "2746   65521  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities[df_cities['Number of Veterans'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los valores perdidos de Number of Veterans y Foreignborn son de Puerto Rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>335559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>76349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>12143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>The Villages</td>\n",
       "      <td>Florida</td>\n",
       "      <td>70.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72590</td>\n",
       "      <td>15231.0</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>The Villages</td>\n",
       "      <td>Florida</td>\n",
       "      <td>70.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72590</td>\n",
       "      <td>15231.0</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>139967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>The Villages</td>\n",
       "      <td>Florida</td>\n",
       "      <td>70.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72590</td>\n",
       "      <td>15231.0</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>White</td>\n",
       "      <td>72211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>4031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Ponce</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.5</td>\n",
       "      <td>56968.0</td>\n",
       "      <td>64615.0</td>\n",
       "      <td>121583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>120705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>Bayamón</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>39.4</td>\n",
       "      <td>80128.0</td>\n",
       "      <td>90131.0</td>\n",
       "      <td>170259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>169155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>69936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>Guaynabo</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.2</td>\n",
       "      <td>33066.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>70492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>Mayagüez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>35782.0</td>\n",
       "      <td>66581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>65521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              City        State  Median Age  Male Population  \\\n",
       "111       San Juan  Puerto Rico        41.4         155408.0   \n",
       "155         Caguas  Puerto Rico        40.4          34743.0   \n",
       "258       Carolina  Puerto Rico        42.0          64758.0   \n",
       "333   The Villages      Florida        70.5              NaN   \n",
       "449   The Villages      Florida        70.5              NaN   \n",
       "637       Carolina  Puerto Rico        42.0          64758.0   \n",
       "1437  The Villages      Florida        70.5              NaN   \n",
       "1747      San Juan  Puerto Rico        41.4         155408.0   \n",
       "1748      Mayagüez  Puerto Rico        38.1          30799.0   \n",
       "1995         Ponce  Puerto Rico        40.5          56968.0   \n",
       "2004       Bayamón  Puerto Rico        39.4          80128.0   \n",
       "2441      San Juan  Puerto Rico        41.4         155408.0   \n",
       "2589      Guaynabo  Puerto Rico        42.2          33066.0   \n",
       "2597        Caguas  Puerto Rico        40.4          34743.0   \n",
       "2654      Guaynabo  Puerto Rico        42.2          33066.0   \n",
       "2746      Mayagüez  Puerto Rico        38.1          30799.0   \n",
       "\n",
       "      Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "111            186829.0            342237                 NaN           NaN   \n",
       "155             42265.0             77008                 NaN           NaN   \n",
       "258             77308.0            142066                 NaN           NaN   \n",
       "333                 NaN             72590             15231.0        4034.0   \n",
       "449                 NaN             72590             15231.0        4034.0   \n",
       "637             77308.0            142066                 NaN           NaN   \n",
       "1437                NaN             72590             15231.0        4034.0   \n",
       "1747           186829.0            342237                 NaN           NaN   \n",
       "1748            35782.0             66581                 NaN           NaN   \n",
       "1995            64615.0            121583                 NaN           NaN   \n",
       "2004            90131.0            170259                 NaN           NaN   \n",
       "2441           186829.0            342237                 NaN           NaN   \n",
       "2589            37426.0             70492                 NaN           NaN   \n",
       "2597            42265.0             77008                 NaN           NaN   \n",
       "2654            37426.0             70492                 NaN           NaN   \n",
       "2746            35782.0             66581                 NaN           NaN   \n",
       "\n",
       "      Average Household Size State Code                               Race  \\\n",
       "111                      NaN         PR                 Hispanic or Latino   \n",
       "155                      NaN         PR                 Hispanic or Latino   \n",
       "258                      NaN         PR  American Indian and Alaska Native   \n",
       "333                      NaN         FL                 Hispanic or Latino   \n",
       "449                      NaN         FL          Black or African-American   \n",
       "637                      NaN         PR                 Hispanic or Latino   \n",
       "1437                     NaN         FL                              White   \n",
       "1747                     NaN         PR  American Indian and Alaska Native   \n",
       "1748                     NaN         PR                              Asian   \n",
       "1995                     NaN         PR                 Hispanic or Latino   \n",
       "2004                     NaN         PR                 Hispanic or Latino   \n",
       "2441                     NaN         PR                              Asian   \n",
       "2589                     NaN         PR                 Hispanic or Latino   \n",
       "2597                     NaN         PR  American Indian and Alaska Native   \n",
       "2654                     NaN         PR  American Indian and Alaska Native   \n",
       "2746                     NaN         PR                 Hispanic or Latino   \n",
       "\n",
       "       Count  \n",
       "111   335559  \n",
       "155    76349  \n",
       "258    12143  \n",
       "333     1066  \n",
       "449      331  \n",
       "637   139967  \n",
       "1437   72211  \n",
       "1747    4031  \n",
       "1748     235  \n",
       "1995  120705  \n",
       "2004  169155  \n",
       "2441    2452  \n",
       "2589   69936  \n",
       "2597     624  \n",
       "2654     589  \n",
       "2746   65521  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities[df_cities['Average Household Size'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities[df_cities.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cities_data(df_cities):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Modify the names of the columns to more descriptive values\n",
    "    2. Fix the missing values issues\n",
    "    3. Modify data types\n",
    "    \n",
    "    INPUT:\n",
    "    df_cities (Spark DataFrame): DataFrame directly extracted from the raw data without any cleaning process\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_cities (Spark DataFrame): data already cleaned for being uploaded to the data lake\n",
    "    '''\n",
    "    \n",
    "    # 1. Modify the names of the columns to more descriptive values\n",
    "    df_cities.columns = df_cities.columns.str.lower()\n",
    "    df_cities.columns = df_cities.columns.str.replace(\" \",\"_\")\n",
    "    df_cities.columns = df_cities.columns.str.replace(\"foreign-born\",\"foreign_born\")\n",
    "    \n",
    "    # 2. Fix the missing values issues\n",
    "    # median age\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['median_age'].mean()>0:\n",
    "            df_cities['median_age'] = df_cities.groupby('state')['median_age'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['median_age'] = df_cities.groupby('state')['median_age'].transform(lambda x: x.fillna(df_cities.median_age.mean()))\n",
    "            \n",
    "    # male population\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['male_population'].mean()>0:\n",
    "            df_cities['male_population'] = df_cities.groupby('state')['male_population'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities.male_population = df_cities.groupby('state')['male_population'].transform(lambda x: x.fillna(df_cities.male_population.mean()))\n",
    "            \n",
    "    # female population\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['female_population'].mean()>0:\n",
    "            df_cities['female_population'] = df_cities.groupby('state')['female_population'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities.male_population = df_cities.groupby('state')['female_population'].transform(lambda x: x.fillna(df_cities.female_population.mean()))\n",
    "    \n",
    "    # total population\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['total_population'].mean()>0:\n",
    "            df_cities['total_population'] = df_cities.groupby('state')['total_population'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['total_population'] = df_cities.groupby('state')['total_population'].transform(lambda x: x.fillna(df_cities.total_population.mean()))    \n",
    "            \n",
    "    # number of veterans\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['number_of_veterans'].mean()>0:\n",
    "            df_cities['number_of_veterans'] = df_cities.groupby('state')['number_of_veterans'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['number_of_veterans'] = df_cities.groupby('state')['number_of_veterans'].transform(lambda x: x.fillna(df_cities.number_of_veterans.mean()))\n",
    "    \n",
    "    # foreign born\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['foreign_born'].mean()>0:\n",
    "            df_cities['foreign_born'] = df_cities.groupby('state')['foreign_born'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['foreign_born'] = df_cities.groupby('state')['foreign_born'].transform(lambda x: x.fillna(df_cities.foreign_born.mean()))\n",
    "    \n",
    "    # average household size\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['average_household_size'].mean()>0:\n",
    "            df_cities['average_household_size'] = df_cities.groupby('state')['average_household_size'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities.average_household_size = df_cities.groupby('state')['average_household_size'].transform(lambda x: x.fillna(df_cities.average_household_size.mean()))\n",
    "    \n",
    "    # count\n",
    "    list_states = list(df_cities.state.unique())\n",
    "    for i in list_states:\n",
    "        if df_cities[df_cities.state==i]['count'].mean()>0:\n",
    "            df_cities['count'] = df_cities.groupby('state')['count'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        else:\n",
    "            df_cities['count'] = df_cities.groupby('state')['count'].transform(lambda x: x.fillna(df_cities.count.mean()))\n",
    "    \n",
    "    # Drop the lines of the columns which have key values that can not be infered\n",
    "    df_cities.dropna(subset = ['city','state','state_code','race'], inplace = True)\n",
    "    \n",
    "    # 3. Modify data types\n",
    "    list_float_to_int = ['male_population','female_population','total_population','number_of_veterans',\n",
    "                    'foreign_born','count']\n",
    "\n",
    "    for col in list_float_to_int:\n",
    "        df_cities[col] = df_cities[col].astype('int')\n",
    "\n",
    "    # 4. Replace codes with more descriptive values\n",
    "    \n",
    "    \n",
    "    # 5. Drop duplicated values\n",
    "    \n",
    "    return df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>race</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city          state  median_age  male_population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   female_population  total_population  number_of_veterans  foreign_born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   average_household_size state_code                       race  count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities = cleaning_cities_data(df_cities)\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities.to_csv(output_data + immigration/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27780/3731543035.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_cities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;31m# open URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    559\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             file_obj = fsspec.open(\n\u001b[0m\u001b[0;32m    334\u001b[0m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfsspec_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             ).open()\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\fsspec\\core.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(urlpath, mode, compression, encoding, errors, protocol, newline, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOpenFile\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \"\"\"\n\u001b[1;32m--> 419\u001b[1;33m     return open_files(\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[0murlpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0murlpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\fsspec\\core.py\u001b[0m in \u001b[0;36mopen_files\u001b[1;34m(urlpath, mode, compression, encoding, errors, name_function, num, protocol, newline, auto_mkdir, expand, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mauto_mkdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mparents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m     return OpenFiles(\n\u001b[0;32m    285\u001b[0m         [\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\fsspec\\core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mauto_mkdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mparents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m     return OpenFiles(\n\u001b[0;32m    285\u001b[0m         [\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\fsspec\\asyn.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\fsspec\\asyn.py\u001b[0m in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mFSTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\fsspec\\asyn.py\u001b[0m in \u001b[0;36m_runner\u001b[1;34m(event, coro, result, timeout)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mcoro\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\s3fs\\core.py\u001b[0m in \u001b[0;36m_makedirs\u001b[1;34m(self, path, exist_ok)\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_makedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[1;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_parents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\s3fs\\core.py\u001b[0m in \u001b[0;36m_mkdir\u001b[1;34m(self, path, acl, create_parents, **kwargs)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strip_protocol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m         \u001b[0mbucket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_cities.to_csv(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. World Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = spark.read.csv(\"C:/Users/gonza/Downloads/GlobalLandTemperaturesByCity.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_eeuu = df_temp.where(df_temp.Country == \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_eeuu.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_eeuu.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_eeuu.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_temperature_data(df_temp):\n",
    "    '''\n",
    "    Function which cleans the data implementing the following steps: \n",
    "    1. Modify the data types\n",
    "    2. Fix the missing values issues\n",
    "    3. Drop unnecessary columns\n",
    "    \n",
    "    INPUT:\n",
    "    df_temp (Spark DataFrame): DataFrame directly extracted from the raw data without any cleaning process\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_temp_eeuu (Spark DataFrame): data already cleaned for being uploaded to the data lake\n",
    "    '''\n",
    "    df_temp_eeuu = df_temp.where(df_temp.Country == \"United States\")\n",
    "    \n",
    "    # 1. Modify the data types\n",
    "    \n",
    "    # Change the datastamp format, since the hour, minutes and seconds are not necessary and do not contribute to the final result\n",
    "    df_temp_eeuu = df_temp_eeuu.withColumn('dt', to_date('dt'))\n",
    "    \n",
    "    # 2. Fix the missing values issues\n",
    "    df_pandas = df_temp_eeuu.toPandas()\n",
    "    df_pandas['AverageTemperature'] = df_pandas.groupby('City')['AverageTemperature'].transform(lambda x: x.fillna(x.mean()))\n",
    "    df_pandas['AverageTemperatureUncertainty'] = df_pandas.groupby('City')['AverageTemperatureUncertainty'].transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # 3. Drop unnecessary columns\n",
    "    df_pandas.drop(\"Country\", axis = 1, inplace = True)\n",
    "    \n",
    "    return df_temp_eeuu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = read_temp_data() #OK, pero el toPandas no funciona y no entiendo el por qué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_eeuu = cleaning_temperature_data(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_temp_eeuu.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_data = \"s3a://bucket-test-udacity/\"\n",
    "df_temp_eeuu.write.partitionBy('dt').mode('overwrite').csv(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
